{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-openai\n",
      "Version: 0.1.23\n",
      "Summary: An integration package connecting OpenAI and LangChain\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: c:\\miniconda\\Gen_AI_Automation_Deep_Learning\\LangChain_Mastery_Develop_LLM_Apps_with_LangChain_&_Pinecone\\venv\\Lib\\site-packages\n",
      "Requires: langchain-core, openai, tiktoken\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum mechanics is a fundamental theory in physics that describes the behavior of matter and energy at the smallest scales, where particles exhibit both wave-like and particle-like properties, and their states are fundamentally probabilistic rather than deterministic.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke(\"Explain Quantum Mechanics in one sentence\", model = \"gpt-4o-mini\")\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantenmechanik beschreibt das Verhalten von Teilchen auf atomarer und subatomarer Ebene durch probabilistische Wellenfunktionen.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import(\n",
    "    SystemMessage, \n",
    "    AIMessage, \n",
    "    HumanMessage\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content = \"You are a physicist and respond only in German\"),\n",
    "    HumanMessage(content=\"Explain quantum mechanics in one sentence\")\n",
    "]\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching LLM Responses\n",
    "- Using Langchain's `InMemoryCache()`\n",
    "- Using `SQLiteCache()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name = \"gpt-3.5-turbo-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.cache import InMemoryCache\n",
    "set_llm_cache(InMemoryCache())\n",
    "prompt = \"Explain quantum mechanics in 5 words\"\n",
    "output = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Wave-like behavior of subatomic particles.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWave-like behavior of subatomic particles.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "set_llm_cache(SQLiteCache(database_path = \".langchain.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 717 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two-tired. \""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# First request (not in cache, takes longer)\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 344 ms\n",
      "Wall time: 361 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two-tired. \""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Second request (cached, faster)\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Title: \"Raven's Moonlight\"**\n",
      "\n",
      "(Verse 1)  \n",
      "Underneath the silver glow, where shadows start to creep,  \n",
      "A raven calls from the edge of night, secrets it will keep.  \n",
      "With wings like midnight feathers, it dances on the breeze,  \n",
      "Whispering to the moonlit sky, a song that brings me to my knees.  \n",
      "The stars like diamonds flicker, in a canvas dark and deep,  \n",
      "As the raven soars through the night, while the world around me sleeps.  \n",
      "\n",
      "(Chorus)  \n",
      "Oh, raven of the moonlight, take me where you fly,  \n",
      "Through the mysteries of the night, beneath the endless sky.  \n",
      "We'll chase the dreams that haunt us, with every haunting cry,  \n",
      "In the shadows of the moonlight, together we will rise.  \n",
      "With the fire of the night, and the howl of the wind,  \n",
      "We'll carve our names in starlight, let the wild ride begin!  \n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name = \"gpt-4o-mini\")\n",
    "prompt = \"Write a rock song about the moon and a raven (2 paras)\"\n",
    "print(llm.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main difference between `ChatOpenAI` and `OpenAI` in the langchain_openai library lies in their ***usage and functionality***. `ChatOpenAI` is specifically *designed for chat models*, where the input and output are ***conversational messages***. On the other hand, `OpenAI` is used for *interfacing with language models* (LMS) and is ***NOT specifically tailored for chat interactions***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Verse 1**  \n",
      "In the shadow of the night, where the silver light glows,  \n",
      "A raven takes to flight, with secrets only it knows.  \n",
      "Wings like whispers, cutting through the dark,  \n",
      "It dances with the moon, leaving behind its mark.  \n",
      "Howling winds, they call, while the stars ignite the sky,  \n",
      "Together they weave tales of love and lullabies.  \n",
      "\n",
      "**Chorus**  \n",
      "Oh, moonlight dreams and midnight schemes,  \n",
      "With a raven soaring high, breaking all the seams.  \n",
      "In a world of shadows, where the wild hearts roam,  \n",
      "We’ll chase the night together, never far from home.  "
     ]
    }
   ],
   "source": [
    "for i in llm.stream(prompt):\n",
    "    print(i.content, end = \"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an experienced virologist.\\nWrite a few sentences about the following virus \"Monkey Pox\" in English'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = '''You are an experienced virologist.\n",
    "Write a few sentences about the following virus \"{virus}\" in {language}'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template = template)\n",
    "\n",
    "prompt = prompt_template.format(virus = \"Monkey Pox\", language = \"English\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monkeypox is a viral zoonotic disease caused by the monkeypox virus, which is part of the Orthopoxvirus genus, the same family that includes smallpox. Initially identified in laboratory monkeys in 1958, the virus primarily circulates in certain animal populations in Central and West Africa, but human cases have been reported outside these regions. Transmission to humans can occur through direct contact with infected animals, bodily fluids, or contaminated materials, and it can also spread between humans through respiratory droplets or skin lesions. Symptoms typically include fever, rash, and swollen lymph nodes, and while the disease is generally self-limiting, it can lead to severe complications in some cases. Vaccination against smallpox has been shown to provide some cross-protection against monkeypox."
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name = \"gpt-4o-mini\", temperature=0)\n",
    "output = llm.invoke(prompt).content\n",
    "\n",
    "for i in llm.stream(prompt):\n",
    "    print(i.content, end = \"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatPromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You respond only in the JSON format.'), HumanMessage(content='Top 5 countries in World by population.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# Create a chat template with system and human messages\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content='You respond only in the JSON format.'),\n",
    "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population.')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fill in the specific values for n and area\n",
    "messages = chat_template.format_messages(n='5', area='World')\n",
    "print(messages)  # Outputs the formatted chat messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"countries\": [\n",
      "        {\n",
      "            \"rank\": 1,\n",
      "            \"country\": \"China\",\n",
      "            \"population\": \"1,439,323,776\"\n",
      "        },\n",
      "        {\n",
      "            \"rank\": 2,\n",
      "            \"country\": \"India\",\n",
      "            \"population\": \"1,380,004,385\"\n",
      "        },\n",
      "        {\n",
      "            \"rank\": 3,\n",
      "            \"country\": \"United States\",\n",
      "            \"population\": \"331,002,651\"\n",
      "        },\n",
      "        {\n",
      "            \"rank\": 4,\n",
      "            \"country\": \"Indonesia\",\n",
      "            \"population\": \"273,523,615\"\n",
      "        },\n",
      "        {\n",
      "            \"rank\": 5,\n",
      "            \"country\": \"Pakistan\",\n",
      "            \"population\": \"220,892,340\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshat Shethia\\AppData\\Local\\Temp\\ipykernel_23320\\4130418131.py:9: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  chain = LLMChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an experienced Virologist. Write a few Sentences about the following virus 'Covid-19' in 'english'.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'virus': 'Covid-19', 'language': 'english', 'text': 'COVID-19, caused by the novel coronavirus SARS-CoV-2, emerged in late 2019 and quickly led to a global pandemic. The virus primarily spreads through respiratory droplets when an infected person coughs, sneezes, or talks. Symptoms can range from mild respiratory issues to severe pneumonia, and some individuals may remain asymptomatic. Vaccination efforts have been crucial in controlling the spread of the virus and reducing severe illness. Ongoing research continues to monitor variants and their potential impact on transmissibility and vaccine efficacy.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o-mini\")\n",
    "template = \"You are an experienced Virologist. Write a few Sentences about the following virus '{virus}' in '{language}'.\"\n",
    "prompt_template = PromptTemplate.from_template(template = template)\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = prompt_template,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "ouptut = chain.invoke({\"virus\": \"Covid-19\", \"language\": \"english\"})\n",
    "print(ouptut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWhat is the capital of India? List the top 3 places to visit in that country. Use Bullet Points\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The capital of India is New Delhi. Here are the top 3 places to visit in the country:\n",
      "\n",
      "- **Taj Mahal (Agra)**: A UNESCO World Heritage Site and an iconic symbol of love, this stunning marble mausoleum attracts millions of visitors each year.\n",
      "  \n",
      "- **Jaipur (Rajasthan)**: Known as the \"Pink City,\" Jaipur is famous for its majestic forts, palaces, and vibrant markets, including the Hawa Mahal and Amber Fort.\n",
      "  \n",
      "- **Varanasi (Uttar Pradesh)**: One of the oldest living cities in the world, Varanasi is a significant cultural and spiritual center, known for its ghats along the Ganges River and its rich traditions.\n"
     ]
    }
   ],
   "source": [
    "template = \"What is the capital of {country}? List the top 3 places to visit in that country. Use Bullet Points\"\n",
    "prompt_template = PromptTemplate.from_template(template = template)\n",
    "chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = prompt_template,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "country = input(\"Enter the name of a country: \")\n",
    "output = chain.invoke(country)\n",
    "print(output[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mCertainly! Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. In its simplest form, simple linear regression involves fitting a straight line to a set of data points.\n",
      "\n",
      "Here's a Python function that implements simple linear regression using the least squares method. This function will calculate the slope and intercept of the regression line and can also make predictions based on the model.\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def linear_regression(x, y):\n",
      "    \"\"\"\n",
      "    Perform simple linear regression on the given data.\n",
      "\n",
      "    Parameters:\n",
      "    x (array-like): Independent variable (predictor).\n",
      "    y (array-like): Dependent variable (response).\n",
      "\n",
      "    Returns:\n",
      "    slope (float): Slope of the regression line.\n",
      "    intercept (float): Intercept of the regression line.\n",
      "    predictions (array): Predicted values based on the regression line.\n",
      "    \"\"\"\n",
      "    # Convert x and y to numpy arrays\n",
      "    x = np.array(x)\n",
      "    y = np.array(y)\n",
      "\n",
      "    # Calculate the number of observations\n",
      "    n = len(x)\n",
      "\n",
      "    # Calculate the slope (m) and intercept (b)\n",
      "    x_mean = np.mean(x)\n",
      "    y_mean = np.mean(y)\n",
      "\n",
      "    # Using the least squares formula\n",
      "    numerator = np.sum((x - x_mean) * (y - y_mean))\n",
      "    denominator = np.sum((x - x_mean) ** 2)\n",
      "\n",
      "    slope = numerator / denominator\n",
      "    intercept = y_mean - (slope * x_mean)\n",
      "\n",
      "    # Calculate predictions\n",
      "    predictions = slope * x + intercept\n",
      "\n",
      "    return slope, intercept, predictions\n",
      "\n",
      "# Example usage\n",
      "if __name__ == \"__main__\":\n",
      "    # Sample data\n",
      "    x = [1, 2, 3, 4, 5]\n",
      "    y = [2, 3, 5, 7, 11]\n",
      "\n",
      "    # Perform linear regression\n",
      "    slope, intercept, predictions = linear_regression(x, y)\n",
      "\n",
      "    print(f\"Slope: {slope}\")\n",
      "    print(f\"Intercept: {intercept}\")\n",
      "\n",
      "    # Plotting the results\n",
      "    plt.scatter(x, y, color='blue', label='Data Points')\n",
      "    plt.plot(x, predictions, color='red', label='Regression Line')\n",
      "    plt.xlabel('Independent Variable (x)')\n",
      "    plt.ylabel('Dependent Variable (y)')\n",
      "    plt.title('Simple Linear Regression')\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **Function Definition**: The `linear_regression` function takes two parameters, `x` and `y`, which are the independent and dependent variables, respectively.\n",
      "2. **Mean Calculation**: It calculates the means of `x` and `y`.\n",
      "3. **Slope and Intercept Calculation**: It uses the least squares method to determine the slope and intercept of the regression line.\n",
      "4. **Predictions**: The function calculates the predicted values based on the regression line.\n",
      "5. **Example Usage**: The script includes an example dataset and demonstrates how to call the function, print the results, and plot the data along with the regression line.\n",
      "\n",
      "You can run this code in a Python environment with `numpy` and `matplotlib` installed to see the results visually.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThe provided Python function `linear_regression` implements a simple form of linear regression, which is a core statistical method for modeling the relationship between a dependent variable (often referred to as the response or outcome variable) and one independent variable (the predictor or feature). Let’s dive into the various components of the function, explaining how they operate and their significance.\n",
      "\n",
      "### Function Breakdown\n",
      "\n",
      "1. **Imports**:\n",
      "   ```python\n",
      "   import numpy as np\n",
      "   import matplotlib.pyplot as plt\n",
      "   ```\n",
      "   - **NumPy**: A fundamental package for scientific computing in Python. It provides support for arrays and mathematical functions.\n",
      "   - **Matplotlib**: A plotting library in Python for data visualization. It is used here to visualize the data points and the regression line.\n",
      "\n",
      "2. **Function Definition**:\n",
      "   ```python\n",
      "   def linear_regression(x, y):\n",
      "   ```\n",
      "   - The function `linear_regression` takes in two arguments: `x` and `y`, which should contain the independent and dependent variable data, respectively.\n",
      "\n",
      "3. **Convert to NumPy Arrays**:\n",
      "   ```python\n",
      "   x = np.array(x)\n",
      "   y = np.array(y)\n",
      "   ```\n",
      "   - Both `x` and `y` are converted into NumPy arrays for easier manipulation and operations. This allows for element-wise calculations when necessary.\n",
      "\n",
      "4. **Calculate Number of Observations**:\n",
      "   ```python\n",
      "   n = len(x)\n",
      "   ```\n",
      "   - The number of data points, `n`, is determined by finding the length of the array `x`.\n",
      "\n",
      "5. **Calculate Means**:\n",
      "   ```python\n",
      "   x_mean = np.mean(x)\n",
      "   y_mean = np.mean(y)\n",
      "   ```\n",
      "   - The mean (average) of each array is computed, which will be used in calculating the slope and intercept of the regression line.\n",
      "\n",
      "6. **Compute Slope and Intercept**:\n",
      "   - **Slope Calculation**:\n",
      "     ```python\n",
      "     numerator = np.sum((x - x_mean) * (y - y_mean))\n",
      "     denominator = np.sum((x - x_mean) ** 2)\n",
      "\n",
      "     slope = numerator / denominator\n",
      "     ```\n",
      "     - The **numerator** represents the covariance between `x` and `y`, calculated as the sum of the products of the deviations of each data point from their respective means.\n",
      "     - The **denominator** is the variance of `x`, calculated as the sum of the squares of the deviations of `x` from its mean.\n",
      "     - The slope `m` of the regression line is derived from the ratio of covariance to variance.\n",
      "\n",
      "   - **Intercept Calculation**:\n",
      "     ```python\n",
      "     intercept = y_mean - (slope * x_mean)\n",
      "     ```\n",
      "     - The intercept `b` is determined by adjusting the mean of `y` based on how much `x` contributes to `y` through the slope.\n",
      "\n",
      "7. **Predictions**:\n",
      "   ```python\n",
      "   predictions = slope * x + intercept\n",
      "   ```\n",
      "   - This formula uses the linear equation \\( y = mx + b \\) to compute predicted values based on the fitted regression line for each `x` data point.\n",
      "\n",
      "8. **Return Values**:\n",
      "   ```python\n",
      "   return slope, intercept, predictions\n",
      "   ```\n",
      "   - The function returns three primary pieces of information: the calculated slope, intercept, and the predicted values.\n",
      "\n",
      "### Example Usage\n",
      "\n",
      "- In the main script:\n",
      "  ```python\n",
      "  if __name__ == \"__main__\":\n",
      "  ```\n",
      "  - This block ensures that the code runs only if the script is executed directly, facilitating better modular programming.\n",
      "\n",
      "- **Sample Data**:\n",
      "  ```python\n",
      "  x = [1, 2, 3, 4, 5]\n",
      "  y = [2, 3, 5, 7, 11]\n",
      "  ```\n",
      "  - A small dataset of `x` and `y` values is presented as an example for regression analysis.\n",
      "\n",
      "- **Perform Linear Regression**:\n",
      "  ```python\n",
      "  slope, intercept, predictions = linear_regression(x, y)\n",
      "  ```\n",
      "  - Calling the `linear_regression` function processes the data and returns the slope, intercept, and predictions.\n",
      "\n",
      "- **Output**:\n",
      "  ```python\n",
      "  print(f\"Slope: {slope}\")\n",
      "  print(f\"Intercept: {intercept}\")\n",
      "  ```\n",
      "  - The slope and intercept values are printed, indicating the relationship's parameters.\n",
      "\n",
      "### Data Visualization\n",
      "\n",
      "- **Plotting**:\n",
      "  ```python\n",
      "  plt.scatter(x, y, color='blue', label='Data Points')\n",
      "  plt.plot(x, predictions, color='red', label='Regression Line')\n",
      "  plt.xlabel('Independent Variable (x)')\n",
      "  plt.ylabel('Dependent Variable (y)')\n",
      "  plt.title('Simple Linear Regression')\n",
      "  plt.legend()\n",
      "  plt.show()\n",
      "  ```\n",
      "  - The data points are displayed as blue dots, while the fitted regression line comes in red.\n",
      "  - The plot includes axis labels and a title, along with a legend that distinguishes between the data points and regression line.\n",
      "\n",
      "### Conclusion\n",
      "This function showcases essential concepts of linear regression, including data processing, statistical calculations, and visualization, all while employing common Python libraries. It serves as a sound example of how to implement a foundational machine learning algorithm in Python succinctly, providing insights into the relationships of bivariate datasets systematically. By running this code, users can evaluate their results and understand the implementation of simple linear regression.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "# Initialize the first ChatOpenAI model (gpt-4o-mini) with specific temperature\n",
    "llm1 = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.5)\n",
    "\n",
    "# Define the first prompt template\n",
    "prompt_template1 = PromptTemplate.from_template(\n",
    "    template='You are an experienced scientist and Python programmer. Write a function that implements the concept of {concept}.'\n",
    ")\n",
    "# Create an LLMChain using the first model and the prompt template\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt_template1)\n",
    "\n",
    "# Initialize the second ChatOpenAI model (gpt-4o-mini) with specific temperature\n",
    "llm2 = ChatOpenAI(model_name='gpt-4o-mini', temperature=1.2)\n",
    "\n",
    "# Define the second prompt template\n",
    "prompt_template2 = PromptTemplate.from_template(\n",
    "    template='Given the Python function {function}, describe it as detailed as possible.'\n",
    ")\n",
    "# Create another LLMChain using the second model and the prompt template\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt_template2)\n",
    "\n",
    "# Combine both chains into a SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "\n",
    "# Invoke the overall chain with the concept \"linear regression\"\n",
    "output = overall_chain.invoke('linear regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided Python function `linear_regression` implements a simple form of linear regression, which is a core statistical method for modeling the relationship between a dependent variable (often referred to as the response or outcome variable) and one independent variable (the predictor or feature). Let’s dive into the various components of the function, explaining how they operate and their significance.\n",
      "\n",
      "### Function Breakdown\n",
      "\n",
      "1. **Imports**:\n",
      "   ```python\n",
      "   import numpy as np\n",
      "   import matplotlib.pyplot as plt\n",
      "   ```\n",
      "   - **NumPy**: A fundamental package for scientific computing in Python. It provides support for arrays and mathematical functions.\n",
      "   - **Matplotlib**: A plotting library in Python for data visualization. It is used here to visualize the data points and the regression line.\n",
      "\n",
      "2. **Function Definition**:\n",
      "   ```python\n",
      "   def linear_regression(x, y):\n",
      "   ```\n",
      "   - The function `linear_regression` takes in two arguments: `x` and `y`, which should contain the independent and dependent variable data, respectively.\n",
      "\n",
      "3. **Convert to NumPy Arrays**:\n",
      "   ```python\n",
      "   x = np.array(x)\n",
      "   y = np.array(y)\n",
      "   ```\n",
      "   - Both `x` and `y` are converted into NumPy arrays for easier manipulation and operations. This allows for element-wise calculations when necessary.\n",
      "\n",
      "4. **Calculate Number of Observations**:\n",
      "   ```python\n",
      "   n = len(x)\n",
      "   ```\n",
      "   - The number of data points, `n`, is determined by finding the length of the array `x`.\n",
      "\n",
      "5. **Calculate Means**:\n",
      "   ```python\n",
      "   x_mean = np.mean(x)\n",
      "   y_mean = np.mean(y)\n",
      "   ```\n",
      "   - The mean (average) of each array is computed, which will be used in calculating the slope and intercept of the regression line.\n",
      "\n",
      "6. **Compute Slope and Intercept**:\n",
      "   - **Slope Calculation**:\n",
      "     ```python\n",
      "     numerator = np.sum((x - x_mean) * (y - y_mean))\n",
      "     denominator = np.sum((x - x_mean) ** 2)\n",
      "\n",
      "     slope = numerator / denominator\n",
      "     ```\n",
      "     - The **numerator** represents the covariance between `x` and `y`, calculated as the sum of the products of the deviations of each data point from their respective means.\n",
      "     - The **denominator** is the variance of `x`, calculated as the sum of the squares of the deviations of `x` from its mean.\n",
      "     - The slope `m` of the regression line is derived from the ratio of covariance to variance.\n",
      "\n",
      "   - **Intercept Calculation**:\n",
      "     ```python\n",
      "     intercept = y_mean - (slope * x_mean)\n",
      "     ```\n",
      "     - The intercept `b` is determined by adjusting the mean of `y` based on how much `x` contributes to `y` through the slope.\n",
      "\n",
      "7. **Predictions**:\n",
      "   ```python\n",
      "   predictions = slope * x + intercept\n",
      "   ```\n",
      "   - This formula uses the linear equation \\( y = mx + b \\) to compute predicted values based on the fitted regression line for each `x` data point.\n",
      "\n",
      "8. **Return Values**:\n",
      "   ```python\n",
      "   return slope, intercept, predictions\n",
      "   ```\n",
      "   - The function returns three primary pieces of information: the calculated slope, intercept, and the predicted values.\n",
      "\n",
      "### Example Usage\n",
      "\n",
      "- In the main script:\n",
      "  ```python\n",
      "  if __name__ == \"__main__\":\n",
      "  ```\n",
      "  - This block ensures that the code runs only if the script is executed directly, facilitating better modular programming.\n",
      "\n",
      "- **Sample Data**:\n",
      "  ```python\n",
      "  x = [1, 2, 3, 4, 5]\n",
      "  y = [2, 3, 5, 7, 11]\n",
      "  ```\n",
      "  - A small dataset of `x` and `y` values is presented as an example for regression analysis.\n",
      "\n",
      "- **Perform Linear Regression**:\n",
      "  ```python\n",
      "  slope, intercept, predictions = linear_regression(x, y)\n",
      "  ```\n",
      "  - Calling the `linear_regression` function processes the data and returns the slope, intercept, and predictions.\n",
      "\n",
      "- **Output**:\n",
      "  ```python\n",
      "  print(f\"Slope: {slope}\")\n",
      "  print(f\"Intercept: {intercept}\")\n",
      "  ```\n",
      "  - The slope and intercept values are printed, indicating the relationship's parameters.\n",
      "\n",
      "### Data Visualization\n",
      "\n",
      "- **Plotting**:\n",
      "  ```python\n",
      "  plt.scatter(x, y, color='blue', label='Data Points')\n",
      "  plt.plot(x, predictions, color='red', label='Regression Line')\n",
      "  plt.xlabel('Independent Variable (x)')\n",
      "  plt.ylabel('Dependent Variable (y)')\n",
      "  plt.title('Simple Linear Regression')\n",
      "  plt.legend()\n",
      "  plt.show()\n",
      "  ```\n",
      "  - The data points are displayed as blue dots, while the fitted regression line comes in red.\n",
      "  - The plot includes axis labels and a title, along with a legend that distinguishes between the data points and regression line.\n",
      "\n",
      "### Conclusion\n",
      "This function showcases essential concepts of linear regression, including data processing, statistical calculations, and visualization, all while employing common Python libraries. It serves as a sound example of how to implement a foundational machine learning algorithm in Python succinctly, providing insights into the relationships of bivariate datasets systematically. By running this code, users can evaluate their results and understand the implementation of simple linear regression.\n"
     ]
    }
   ],
   "source": [
    "print(output[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  LangChain Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-experimental\n",
      "Version: 0.0.64\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: c:\\miniconda\\Gen_AI_Automation_Deep_Learning\\LangChain_Mastery_Develop_LLM_Apps_with_LangChain_&_Pinecone\\venv\\Lib\\site-packages\n",
      "Requires: langchain-community, langchain-core\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[13, 26, 39, 52, 65, 78, 91]\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.utilities import PythonREPL\n",
    "python_repl = PythonREPL()\n",
    "python_repl.run('print([n for n in range(1, 100) if n % 13 == 0])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo calculate 2 + 2 using Python, I will execute the addition operation in the Python REPL.  \n",
      "Action: Python_REPL  \n",
      "Action Input: print(2 + 2)  \u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m4\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer  \n",
      "Final Answer: 4\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Calculate 2 + 2 using Python', 'output': '4'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = 'Calculate 2 + 2 using Python'\n",
    "agent_executor.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo calculate the square root of the factorial of 12, I need to first compute the factorial of 12 and then take the square root of that result. I will use the `math` module in Python, which provides a function for calculating factorials and another for calculating square roots.\n",
      "\n",
      "Action: Python_REPL  \n",
      "Action Input: `import math; result = math.sqrt(math.factorial(12)); result`  \u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to print the result to see the output.  \n",
      "Action: Python_REPL  \n",
      "Action Input: `import math; result = math.sqrt(math.factorial(12)); print(result)`  \u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m21886.105181141756\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer  \n",
      "Final Answer: 21886.105181141756\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Calculate the square root of the factorial of 12 using Python',\n",
       " 'output': '21886.105181141756'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = 'Calculate the square root of the factorial of 12 using Python'\n",
    "agent_executor.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo calculate the square root of the factorial of 12, I will first compute the factorial of 12, then take the square root of that result. Finally, I will format the output to display it with 4 decimal points. \n",
      "\n",
      "Action: Python_REPL  \n",
      "Action Input: `import math; result = math.sqrt(math.factorial(12)); format_result = format(result, '.4f'); format_result`  \u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to check the output of the code to see the result of the calculation. \n",
      "\n",
      "Action: Python_REPL  \n",
      "Action Input: `import math; result = math.sqrt(math.factorial(12)); format_result = format(result, '.4f'); format_result`  \u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `I don't know`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\miniconda\\Gen_AI_Automation_Deep_Learning\\LangChain_Mastery_Develop_LLM_Apps_with_LangChain_&_Pinecone\\venv\\Lib\\site-packages\\langchain\\agents\\agent.py:1359\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m-> 1359\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1364\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\miniconda\\Gen_AI_Automation_Deep_Learning\\LangChain_Mastery_Develop_LLM_Apps_with_LangChain_&_Pinecone\\venv\\Lib\\site-packages\\langchain\\agents\\agent.py:807\u001b[0m, in \u001b[0;36mAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    806\u001b[0m full_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfull_inputs)\n\u001b[1;32m--> 807\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\Gen_AI_Automation_Deep_Learning\\LangChain_Mastery_Develop_LLM_Apps_with_LangChain_&_Pinecone\\venv\\Lib\\site-packages\\langchain\\agents\\mrkl\\output_parser.py:79\u001b[0m, in \u001b[0;36mMRKLOutputParser.parse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*:[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*(.*?)\u001b[39m\u001b[38;5;124m\"\u001b[39m, text, re\u001b[38;5;241m.\u001b[39mDOTALL):\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse LLM output: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     81\u001b[0m         observation\u001b[38;5;241m=\u001b[39mMISSING_ACTION_AFTER_THOUGHT_ERROR_MESSAGE,\n\u001b[0;32m     82\u001b[0m         llm_output\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m     83\u001b[0m         send_to_llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     84\u001b[0m     )\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*Action\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*Input\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*:[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*(.*)\u001b[39m\u001b[38;5;124m\"\u001b[39m, text, re\u001b[38;5;241m.\u001b[39mDOTALL\n\u001b[0;32m     87\u001b[0m ):\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Could not parse LLM output: `I don't know`",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Invoke the agent\u001b[39;00m\n\u001b[0;32m     16\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCalculate the square root of the factorial of 12 and display it with 4 decimal points\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 17\u001b[0m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\Gen_AI_Automation_Deep_Learning\\LangChain_Mastery_Develop_LLM_Apps_with_LangChain_&_Pinecone\\venv\\Lib\\site-packages\\langchain\\chains\\base.py:164\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    163\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    165\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\miniconda\\Gen_AI_Automation_Deep_Learning\\LangChain_Mastery_Develop_LLM_Apps_with_LangChain_&_Pinecone\\venv\\Lib\\site-packages\\langchain\\chains\\base.py:154\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    153\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 154\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    160\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\miniconda\\Gen_AI_Automation_Deep_Learning\\LangChain_Mastery_Develop_LLM_Apps_with_LangChain_&_Pinecone\\venv\\Lib\\site-packages\\langchain\\agents\\agent.py:1625\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1624\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1625\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1631\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1632\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1633\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1634\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1635\u001b[0m         )\n",
      "File \u001b[1;32mc:\\miniconda\\Gen_AI_Automation_Deep_Learning\\LangChain_Mastery_Develop_LLM_Apps_with_LangChain_&_Pinecone\\venv\\Lib\\site-packages\\langchain\\agents\\agent.py:1331\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1324\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1328\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1331\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1334\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1335\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1336\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1337\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1338\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1339\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1340\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1341\u001b[0m     )\n",
      "File \u001b[1;32mc:\\miniconda\\Gen_AI_Automation_Deep_Learning\\LangChain_Mastery_Develop_LLM_Apps_with_LangChain_&_Pinecone\\venv\\Lib\\site-packages\\langchain\\agents\\agent.py:1370\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1368\u001b[0m     raise_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_error:\n\u001b[1;32m-> 1370\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn output parsing error occurred. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to pass this error back to the agent and have it try \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magain, pass `handle_parsing_errors=True` to the AgentExecutor. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1374\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1375\u001b[0m     )\n\u001b[0;32m   1376\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `I don't know`"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o-mini\", temperature = 0)\n",
    "\n",
    "# Create a Python agent using the ChatOpenAI model and a PythonREPLTool\n",
    "agent_executor = create_python_agent(\n",
    "    llm=llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True  # Add this argument to handle parsing errors\n",
    ")\n",
    "\n",
    "# Invoke the agent\n",
    "prompt = 'Calculate the square root of the factorial of 12 and display it with 4 decimal points'\n",
    "agent_executor.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: duckduckgo_search\n",
      "Version: 6.2.11\n",
      "Summary: Search for words, documents, images, news, maps and text translation using the DuckDuckGo.com search engine.\n",
      "Home-page: https://github.com/deedy5/duckduckgo_search\n",
      "Author: deedy5\n",
      "Author-email: \n",
      "License: MIT License\n",
      "Location: c:\\miniconda\\Gen_AI_Automation_Deep_Learning\\LangChain_Mastery_Develop_LLM_Apps_with_LangChain_&_Pinecone\\venv\\Lib\\site-packages\n",
      "Requires: click, primp\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
