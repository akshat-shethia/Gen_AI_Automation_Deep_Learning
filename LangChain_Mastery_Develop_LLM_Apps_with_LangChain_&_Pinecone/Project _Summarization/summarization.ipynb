{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import(\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= r\"\"\"\n",
    "Mojo combines the usability of Python with the performance of C, unlocking unparalleled programmability \\\n",
    "of AI hardware and extensibility of AI models.\n",
    "Mojo is a new programming language that bridges the gap between research and production \\ \n",
    "by combining the best of Python syntax with systems programming and metaprogramming.\n",
    "With Mojo, you can write portable code that’s faster than C and seamlessly inter-op with the Python ecosystem.\n",
    "When we started Modular, we had no intention of building a new programming language. \\\n",
    "But as we were building our platform with the intent to unify the world’s ML/AI infrastructure, \\\n",
    "we realized that programming across the entire stack was too complicated. Plus, we were writing a \\\n",
    "lot of MLIR by hand and not having a good time.\n",
    "And although accelerators are important, one of the most prevalent and sometimes overlooked \"accelerators\" \\\n",
    "is the host CPU. Nowadays, CPUs have lots of tensor-core-like accelerator blocks and other AI acceleration \\\n",
    "units, but they also serve as the “fallback” for operations that specialized accelerators don’t handle, \\\n",
    "such as data loading, pre- and post-processing, and integrations with foreign systems. \\\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are an expert copywriter with expertize in summarizing documents'),\n",
    "    HumanMessage(content=f'Please provide a short and concise summary of the following text:\\n TEXT: {text}')\n",
    "]\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mojo is a new programming language that merges Python's usability with C's performance, enhancing AI hardware programmability and model extensibility. It aims to simplify the transition from research to production by offering a syntax similar to Python while incorporating systems programming and metaprogramming. Mojo enables the creation of portable, high-speed code that integrates well with the Python ecosystem. The development of Mojo arose from the challenges faced while building a unified ML/AI infrastructure, highlighting the importance of both specialized accelerators and the host CPU in AI operations.\n"
     ]
    }
   ],
   "source": [
    "summary_output = llm.invoke(messages)\n",
    "print(summary_output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "template = '''\n",
    "Write a concise and short summary of the following text:\n",
    "TEXT: `{text}`\n",
    "Translate the summary to {language}.\n",
    "'''\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['text', 'language'],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(prompt.format(text=text, language='English'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshat Shethia\\AppData\\Local\\Temp\\ipykernel_9352\\3214743786.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt)\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "summary = chain.invoke({'text': text, 'language':'english'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mojo is a new programming language that merges Python's usability with C's performance, enhancing AI hardware programmability and model extensibility. It aims to simplify the complexities of programming across the machine learning and AI infrastructure. Mojo allows for the creation of portable, high-speed code that integrates well with the Python ecosystem. The development of Mojo arose from the challenges faced while building a unified ML/AI platform, highlighting the importance of both specialized accelerators and the host CPU in AI operations.\n"
     ]
    }
   ],
   "source": [
    "print(summary[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "summary = chain.invoke({'text': text, 'language':'hindi'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Summary:**  \n",
      "Mojo is a new programming language that merges Python's usability with C's performance, facilitating AI hardware programmability and model extensibility. It aims to simplify programming across the ML/AI infrastructure by offering portable code that is faster than C and compatible with the Python ecosystem. The creators of Mojo recognized the complexity of programming in this space while developing their platform, leading to the need for a more efficient solution. Additionally, it highlights the importance of host CPUs, which play a crucial role in AI operations beyond specialized accelerators.\n",
      "\n",
      "**Hindi Translation:**  \n",
      "मोजो एक नई प्रोग्रामिंग भाषा है जो पायथन की उपयोगिता को सी की प्रदर्शन के साथ मिलाती है, जिससे एआई हार्डवेयर की प्रोग्रामेबिलिटी और मॉडल की एक्स्टेंसिबिलिटी को बढ़ावा मिलता है। इसका उद्देश्य एमएल/एआई इन्फ्रास्ट्रक्चर में प्रोग्रामिंग को सरल बनाना है, जिससे ऐसा पोर्टेबल कोड लिखा जा सके जो सी से तेज हो और पायथन पारिस्थितिकी तंत्र के साथ संगत हो। मोजो के निर्माताओं ने अपने प्लेटफॉर्म को विकसित करते समय इस क्षेत्र में प्रोग्रामिंग की जटिलता को पहचाना, जिससे एक अधिक कुशल समाधान की आवश्यकता महसूस हुई। इसके अलावा, यह होस्ट सीपीयू के महत्व को उजागर करता है, जो विशेषीकृत त्वरकों के अलावा एआई संचालन में महत्वपूर्ण भूमिका निभाते हैं।\n"
     ]
    }
   ],
   "source": [
    "print(summary[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files/sj.txt', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "docs = [Document(page_content=text)]\n",
    "llm = ChatOpenAI(temperature=0, model_name='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''Write a concise and short summary of the following text.\n",
    "TEXT: `{text}`\n",
    "'''\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['text'],\n",
    "    template=template\n",
    ")\n",
    "prompt\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type='stuff',\n",
    "    prompt=prompt,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_summary = chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a commencement speech, the speaker shares three personal stories that highlight key life lessons. The first story emphasizes the importance of following one's intuition and connecting the dots in hindsight, illustrated by his decision to drop out of college and later apply his calligraphy skills to the design of the Macintosh computer. The second story reflects on love and loss, recounting how being fired from Apple led to a creative resurgence, resulting in the founding of NeXT and Pixar. The third story addresses the inevitability of death, urging graduates to live authentically and pursue their passions without being constrained by others' expectations. He concludes with the mantra \"Stay Hungry. Stay Foolish,\" encouraging graduates to remain curious and adventurous in their lives.\n"
     ]
    }
   ],
   "source": [
    "# output_summary is a dict with 2 keys: 'input_documents' and 'output_text'\n",
    "# displaying the summary\n",
    "print(output_summary['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing Large Documents Using map_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files/sj.txt', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2643"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=50)\n",
    "chunks = text_splitter.create_documents([text])\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type='map_reduce', # Using map_reduce\n",
    "    verbose=False\n",
    ")\n",
    "output_summary = chain.invoke(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In his commencement speech, Steve Jobs shares three key stories from his life. He discusses the importance of connecting past experiences, illustrating how dropping out of college led to his work on the Macintosh's typography. He reflects on love and loss, explaining how being fired from Apple allowed him to pursue fulfilling ventures like NeXT and Pixar. Lastly, he addresses the awareness of mortality, sharing his own cancer diagnosis, which emphasized the need to live authentically and passionately. Jobs encourages graduates to trust their instincts, embrace curiosity, and live boldly, concluding with the message, \"Stay Hungry. Stay Foolish.\"\n"
     ]
    }
   ],
   "source": [
    "# output_summary is a dict with 2 keys: 'input_documents' and 'output_text'\n",
    "# displaying the summary\n",
    "print(output_summary['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
