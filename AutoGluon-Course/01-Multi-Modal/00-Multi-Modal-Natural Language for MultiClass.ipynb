{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982cf445",
   "metadata": {},
   "source": [
    "<a href = \"https://www.pieriantraining.com\"><img src=\"../PT Centered Purple.png\"> </a>\n",
    "\n",
    "<em style=\"text-align:center\">Copyrighted by Pierian Training</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64687c6f",
   "metadata": {},
   "source": [
    "# Multi-Modal: Natural Language for MultiClass\n",
    "\n",
    "Let's explore how to combine tabular data with other modalities, in this case, natural language text reviews. We'll try to predict a customer's 1-5 star review of a book:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5953c1c",
   "metadata": {},
   "source": [
    "We'll be using the data set from: https://www.kaggle.com/code/meetnagadia/amazon-kindle-book-sentiment-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f83d597",
   "metadata": {},
   "source": [
    "We'll treat each star as its own class category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8818965f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c402d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cf9e17",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4ba6495",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TabularDataset(\"data/kindle_review/review.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df973025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['asin', 'rating', 'reviewText', 'reviewerID', 'reviewerName'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d57d54d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>rating</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0033UV8HI</td>\n",
       "      <td>3</td>\n",
       "      <td>Jace Rankin may be short, but he's nothing to ...</td>\n",
       "      <td>A3HHXRELK8BHQG</td>\n",
       "      <td>Ridley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B002HJV4DE</td>\n",
       "      <td>5</td>\n",
       "      <td>Great short read.  I didn't want to put it dow...</td>\n",
       "      <td>A2RGNZ0TRF578I</td>\n",
       "      <td>Holly Butler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B002ZG96I4</td>\n",
       "      <td>3</td>\n",
       "      <td>I'll start by saying this is the first of four...</td>\n",
       "      <td>A3S0H2HV6U1I7F</td>\n",
       "      <td>Merissa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B002QHWOEU</td>\n",
       "      <td>3</td>\n",
       "      <td>Aggie is Angela Lansbury who carries pocketboo...</td>\n",
       "      <td>AC4OQW3GZ919J</td>\n",
       "      <td>Cleargrace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B001A06VJ8</td>\n",
       "      <td>4</td>\n",
       "      <td>I did not expect this type of book to be in li...</td>\n",
       "      <td>A3C9V987IQHOQD</td>\n",
       "      <td>Rjostler</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  rating                                         reviewText  \\\n",
       "0  B0033UV8HI       3  Jace Rankin may be short, but he's nothing to ...   \n",
       "1  B002HJV4DE       5  Great short read.  I didn't want to put it dow...   \n",
       "2  B002ZG96I4       3  I'll start by saying this is the first of four...   \n",
       "3  B002QHWOEU       3  Aggie is Angela Lansbury who carries pocketboo...   \n",
       "4  B001A06VJ8       4  I did not expect this type of book to be in li...   \n",
       "\n",
       "       reviewerID  reviewerName  \n",
       "0  A3HHXRELK8BHQG        Ridley  \n",
       "1  A2RGNZ0TRF578I  Holly Butler  \n",
       "2  A3S0H2HV6U1I7F       Merissa  \n",
       "3   AC4OQW3GZ919J    Cleargrace  \n",
       "4  A3C9V987IQHOQD      Rjostler  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8385ec10",
   "metadata": {},
   "source": [
    "Note how we won't clean up this data or need to get rid of unique identifiers, AutoGluon is smart enough to feature engineer based on detected natural language text and unique values.\n",
    "\n",
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f96158c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data) * 0.8)\n",
    "seed = 42\n",
    "train_data = data.sample(train_size, random_state=seed)\n",
    "test_data = data.drop(train_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bea77727",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'book_rating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "008a63d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TabularPredictor(label=\"rating\", path=save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-marshall",
   "metadata": {},
   "source": [
    "Let's pass **multimodal** to the *hyperparameters* argument to tell autogluon that we want it to use its multimodal tuning procedure.\n",
    "This additionally trains a Transformer Network.\n",
    "\n",
    "Note that you need to have at least one nvidia GPU to be able to train a transformer in autogluon.\n",
    "Otherwise just remove the multimodal parameter and we train autogluon as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22644acb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"book_rating\\\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.12\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Train Data Rows:    9600\n",
      "Train Data Columns: 4\n",
      "Label Column: rating\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\t5 unique label values:  [3, 1, 2, 4, 5]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Train Data Class Count: 5\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    24628.19 MB\n",
      "\tTrain Data (Original)  Memory Usage: 8.62 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\t\tFitting RenameFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['reviewText']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 6657\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('object', [])       : 3 | ['asin', 'reviewerID', 'reviewerName']\n",
      "\t\t('object', ['text']) : 1 | ['reviewText']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :    3 | ['asin', 'reviewerID', 'reviewerName']\n",
      "\t\t('int', ['binned', 'text_special']) :   32 | ['reviewText.char_count', 'reviewText.word_count', 'reviewText.capital_ratio', 'reviewText.lower_ratio', 'reviewText.digit_ratio', ...]\n",
      "\t\t('int', ['text_ngram'])             : 6658 | ['__nlp__.000', '__nlp__.10', '__nlp__.100', '__nlp__.12', '__nlp__.14', ...]\n",
      "\t\t('object', ['text'])                :    1 | ['reviewText_raw_text']\n",
      "\t12.4s = Fit runtime\n",
      "\t4 features in original data used to generate 6694 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 134.81 MB (0.5% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 13.36s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 8640, Val Rows: 960\n",
      "Fitting 8 L1 models ...\n",
      "Fitting model: LightGBM ...\n",
      "\t0.5344\t = Validation score   (accuracy)\n",
      "\t28.8s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.5427\t = Validation score   (accuracy)\n",
      "\t30.27s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tMany features detected (6693), dynamically setting 'colsample_bylevel' to 0.14940983116689077 to speed up training (Default = 1).\n",
      "\tTo disable this functionality, explicitly specify 'colsample_bylevel' in the model hyperparameters.\n",
      "\t0.6073\t = Validation score   (accuracy)\n",
      "\t202.28s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.5385\t = Validation score   (accuracy)\n",
      "\t122.5s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.3542\t = Validation score   (accuracy)\n",
      "\t15.83s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: VowpalWabbit ...\n",
      "\tWarning: Exception caused VowpalWabbit to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import vowpalwabbit` failed.\n",
      "A quick tip is to install via `pip install vowpalwabbit>=9,<9.5\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.5521\t = Validation score   (accuracy)\n",
      "\t131.5s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Fitting model: MultiModalPredictor ...\n",
      "\tWarning: Exception caused MultiModalPredictor to fail during training... Skipping this model.\n",
      "\t\tThe total system num_gpus=0 is less than minimum num_gpus=1 to fit MultiModalPredictorModel. Consider using a machine with more GPUs.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Marcial\\AppData\\Roaming\\Python\\Python39\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1502, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"C:\\Users\\Marcial\\AppData\\Roaming\\Python\\Python39\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1447, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"C:\\Users\\Marcial\\AppData\\Roaming\\Python\\Python39\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 695, in fit\n",
      "    kwargs = self._preprocess_fit_args(**kwargs)\n",
      "  File \"C:\\Users\\Marcial\\AppData\\Roaming\\Python\\Python39\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 428, in _preprocess_fit_args\n",
      "    kwargs = self._preprocess_fit_resources(**kwargs)\n",
      "  File \"C:\\Users\\Marcial\\AppData\\Roaming\\Python\\Python39\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 597, in _preprocess_fit_resources\n",
      "    assert system_num_gpus >= minimum_model_num_gpus, f'The total system num_gpus={system_num_gpus} is less than minimum num_gpus={minimum_model_num_gpus} to fit {self.__class__.__name__}. Consider using a machine with more GPUs.'\n",
      "AssertionError: The total system num_gpus=0 is less than minimum num_gpus=1 to fit MultiModalPredictorModel. Consider using a machine with more GPUs.\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.6177\t = Validation score   (accuracy)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 554.76s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"book_rating\\\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x2048dbaf6a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.fit(train_data, hyperparameters=\"multimodal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "equivalent-malta",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WeightedEnsemble_L2'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.get_model_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5cc8ffb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L2   0.617708       0.487705  232.714246                0.000000           0.162570            2       True          7\n",
      "1             CatBoost   0.607292       0.335099  202.278498                0.335099         202.278498            1       True          3\n",
      "2        LightGBMLarge   0.552083       0.292419  131.497060                0.292419         131.497060            1       True          6\n",
      "3           LightGBMXT   0.542708       0.152606   30.273178                0.152606          30.273178            1       True          2\n",
      "4              XGBoost   0.538542       0.099047  122.499334                0.099047         122.499334            1       True          4\n",
      "5             LightGBM   0.534375       0.171542   28.795342                0.171542          28.795342            1       True          1\n",
      "6       NeuralNetTorch   0.354167       0.019999   15.832055                0.019999          15.832055            1       True          5\n",
      "Number of models trained: 7\n",
      "Types of models trained:\n",
      "{'XGBoostModel', 'CatBoostModel', 'LGBModel', 'TabularNeuralNetTorchModel', 'WeightedEnsembleModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])                    :    3 | ['asin', 'reviewerID', 'reviewerName']\n",
      "('int', ['binned', 'text_special']) :   32 | ['reviewText.char_count', 'reviewText.word_count', 'reviewText.capital_ratio', 'reviewText.lower_ratio', 'reviewText.digit_ratio', ...]\n",
      "('int', ['text_ngram'])             : 6658 | ['__nlp__.000', '__nlp__.10', '__nlp__.100', '__nlp__.12', '__nlp__.14', ...]\n",
      "('object', ['text'])                :    1 | ['reviewText_raw_text']\n",
      "Plot summary of models saved to file: book_rating\\SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    }
   ],
   "source": [
    "predictor.fit_summary();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d28e48b",
   "metadata": {},
   "source": [
    "## Validation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "351f37aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_data[\"rating\"]\n",
    "test_features = test_data.drop(columns=[\"rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fef2962f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = predictor.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2647b4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: accuracy on test data: 0.58375\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"accuracy\": 0.58375,\n",
      "    \"balanced_accuracy\": 0.563489631606563,\n",
      "    \"mcc\": 0.4728101182359241\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "metrics = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13852330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.58375,\n",
       " 'balanced_accuracy': 0.563489631606563,\n",
       " 'mcc': 0.4728101182359241}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b312f",
   "metadata": {},
   "source": [
    "Notice how the accuracy is 60%, this is actually very good for 5 classes, a random guess would be only 20% accurate! Plus this is a difficult problem with 3-4 stars being hard to discern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2af6e6",
   "metadata": {},
   "source": [
    "We can use sklearn to obtain more in-depth metrics like a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "791fcb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ee03eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[263,  89,  21,  18,  17],\n",
       "       [116, 171,  48,  34,  13],\n",
       "       [ 22,  60, 152, 148,  42],\n",
       "       [  8,  14,  56, 329, 165],\n",
       "       [  2,   1,   6, 119, 486]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1965b1f9",
   "metadata": {},
   "source": [
    "We can see that the model mainly had difficulty in trying to differentiate between 3 and 4 as well as 4 and 5, which is very reasonable, as that can also be hard for human to discern and its very subjective!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
