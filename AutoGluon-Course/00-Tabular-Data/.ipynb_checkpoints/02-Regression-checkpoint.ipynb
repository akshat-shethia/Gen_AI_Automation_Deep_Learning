{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9224d92e-02bf-4ea7-b124-b6f9ca1e70c4",
   "metadata": {},
   "source": [
    "<a href = \"https://www.pieriantraining.com\"><img src=\"../PT Centered Purple.png\"> </a>\n",
    "\n",
    "<em style=\"text-align:center\">Copyrighted by Pierian Training</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daaf826-fbab-410b-ac36-97961566d81d",
   "metadata": {},
   "source": [
    "# Tabular Data: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71366cb7-2ce8-4524-b734-31ad10790d6f",
   "metadata": {},
   "source": [
    "In this lecture, you are going to build a regression model to predict UBER fares based on features such as\n",
    "\n",
    "* Pickup Datetime\n",
    "* Picup Coordinates\n",
    "* Dropoff Coordinates\n",
    "* Passenger Count\n",
    "\n",
    "The dataset is based on : https://www.kaggle.com/datasets/yasserh/uber-fares-dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b45e7e6",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36f65913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13933a60",
   "metadata": {},
   "source": [
    "Use **TabularDataset** to load the csv data into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86edda51",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TabularDataset(\"data/uber/uber.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a8725f-8811-4f67-ae59-82244954701f",
   "metadata": {},
   "source": [
    "Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cee539e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.5</td>\n",
       "      <td>2015-05-07 19:52:06 UTC</td>\n",
       "      <td>-73.999817</td>\n",
       "      <td>40.738354</td>\n",
       "      <td>-73.999512</td>\n",
       "      <td>40.723217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2009-07-17 20:04:56 UTC</td>\n",
       "      <td>-73.994355</td>\n",
       "      <td>40.728225</td>\n",
       "      <td>-73.994710</td>\n",
       "      <td>40.750325</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.9</td>\n",
       "      <td>2009-08-24 21:45:00 UTC</td>\n",
       "      <td>-74.005043</td>\n",
       "      <td>40.740770</td>\n",
       "      <td>-73.962565</td>\n",
       "      <td>40.772647</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.3</td>\n",
       "      <td>2009-06-26 08:22:21 UTC</td>\n",
       "      <td>-73.976124</td>\n",
       "      <td>40.790844</td>\n",
       "      <td>-73.965316</td>\n",
       "      <td>40.803349</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2014-08-28 17:47:00 UTC</td>\n",
       "      <td>-73.925023</td>\n",
       "      <td>40.744085</td>\n",
       "      <td>-73.973082</td>\n",
       "      <td>40.761247</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fare_amount          pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
       "0          7.5  2015-05-07 19:52:06 UTC        -73.999817        40.738354   \n",
       "1          7.7  2009-07-17 20:04:56 UTC        -73.994355        40.728225   \n",
       "2         12.9  2009-08-24 21:45:00 UTC        -74.005043        40.740770   \n",
       "3          5.3  2009-06-26 08:22:21 UTC        -73.976124        40.790844   \n",
       "4         16.0  2014-08-28 17:47:00 UTC        -73.925023        40.744085   \n",
       "\n",
       "   dropoff_longitude  dropoff_latitude  passenger_count  \n",
       "0         -73.999512         40.723217                1  \n",
       "1         -73.994710         40.750325                1  \n",
       "2         -73.962565         40.772647                1  \n",
       "3         -73.965316         40.803349                3  \n",
       "4         -73.973082         40.761247                5  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "add6bf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'autogluon.core.dataset.TabularDataset'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   fare_amount        200000 non-null  float64\n",
      " 1   pickup_datetime    200000 non-null  object \n",
      " 2   pickup_longitude   200000 non-null  float64\n",
      " 3   pickup_latitude    200000 non-null  float64\n",
      " 4   dropoff_longitude  199999 non-null  float64\n",
      " 5   dropoff_latitude   199999 non-null  float64\n",
      " 6   passenger_count    200000 non-null  int64  \n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 10.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39ee170f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3633662",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "\n",
    "As our dataset is large, we can train on 90% of the data (180k) and validate on the remaining 20k datapoints. Let's use 42 as our random seed again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cab4220",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 180000\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec3b8329",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.sample(train_size, random_state=seed)\n",
    "test_data = data.drop(train_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2311a4",
   "metadata": {},
   "source": [
    "As previously, we do not need to perform any data preprocessing, AutoGluon is able to recognize the string object as a potential datetime and will perform time feature engineering all on its own!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9b8fa4",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We define the save_path and tell the **TabularPredictor** class the desired label - *fare_amount* in our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "546987fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'uber_predictors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ad6f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TabularPredictor(label=\"fare_amount\", path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059fb709",
   "metadata": {},
   "source": [
    "Let's start the training. Note that it might take some time! \n",
    "It should also be noted that autogluon uses the negative root mean squared error as its validation function, thus the minus sign in the metrics reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e66ae8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (180000 samples, 24.48 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"uber_predictors\\\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.12\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Train Data Rows:    180000\n",
      "Train Data Columns: 6\n",
      "Label Column: fare_amount\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (499.0, -52.0, 11.36729, 9.91752)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    24107.04 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.6 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                        : 1 | ['passenger_count']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['pickup_datetime']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                  : 1 | ['passenger_count']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['pickup_datetime', 'pickup_datetime.year', 'pickup_datetime.month', 'pickup_datetime.day', 'pickup_datetime.dayofweek']\n",
      "\t15.9s = Fit runtime\n",
      "\t6 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 14.4 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 15.93s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.013888888888888888, Train Rows: 177500, Val Rows: 2500\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-10.7906\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-12.1014\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 5.57015\n",
      "[2000]\tvalid_set's rmse: 5.42349\n",
      "[3000]\tvalid_set's rmse: 5.38171\n",
      "[4000]\tvalid_set's rmse: 5.33706\n",
      "[5000]\tvalid_set's rmse: 5.32888\n",
      "[6000]\tvalid_set's rmse: 5.31629\n",
      "[7000]\tvalid_set's rmse: 5.31203\n",
      "[8000]\tvalid_set's rmse: 5.31692\n",
      "[9000]\tvalid_set's rmse: 5.31871\n",
      "[10000]\tvalid_set's rmse: 5.3241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-5.3105\t = Validation score   (-root_mean_squared_error)\n",
      "\t29.71s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 5.3808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-5.3561\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.06s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-5.566\t = Validation score   (-root_mean_squared_error)\n",
      "\t93.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-5.3969\t = Validation score   (-root_mean_squared_error)\n",
      "\t55.51s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-5.9647\t = Validation score   (-root_mean_squared_error)\n",
      "\t24.09s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-9.552\t = Validation score   (-root_mean_squared_error)\n",
      "\t138.23s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-5.4623\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.62s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-6.1794\t = Validation score   (-root_mean_squared_error)\n",
      "\t134.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 5.37828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-5.3672\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.25s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-5.2405\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 517.3s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"uber_predictors\\\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x1d287052550>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362d6eb8-5beb-4e2b-a45d-75e1136f2b03",
   "metadata": {},
   "source": [
    "We can see a summary of the results with: predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab28e77e-3ee1-46de-bf83-9edc65429ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                  model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0   WeightedEnsemble_L2  -5.240455       0.665224  271.915862                0.000998           0.263297            2       True         12\n",
      "1            LightGBMXT  -5.310504       0.483706   29.708837                0.483706          29.708837            1       True          3\n",
      "2              LightGBM  -5.356148       0.046874    6.062788                0.046874           6.062788            1       True          4\n",
      "3         LightGBMLarge  -5.367221       0.051862    8.252931                0.051862           8.252931            1       True         11\n",
      "4              CatBoost  -5.396860       0.004012   55.511079                0.004012          55.511079            1       True          6\n",
      "5               XGBoost  -5.462311       0.025931    6.620389                0.025931           6.620389            1       True          9\n",
      "6       RandomForestMSE  -5.566028       0.068817   93.070806                0.068817          93.070806            1       True          5\n",
      "7         ExtraTreesMSE  -5.964686       0.064847   24.092205                0.064847          24.092205            1       True          7\n",
      "8        NeuralNetTorch  -6.179375       0.012966  134.557203                0.012966         134.557203            1       True         10\n",
      "9       NeuralNetFastAI  -9.551965       0.032912  138.229361                0.032912         138.229361            1       True          8\n",
      "10       KNeighborsUnif -10.790596       0.012141    1.559181                0.012141           1.559181            1       True          1\n",
      "11       KNeighborsDist -12.101352       0.010244    0.601546                0.010244           0.601546            1       True          2\n",
      "Number of models trained: 12\n",
      "Types of models trained:\n",
      "{'RFModel', 'NNFastAiTabularModel', 'TabularNeuralNetTorchModel', 'XGBoostModel', 'XTModel', 'CatBoostModel', 'LGBModel', 'KNNModel', 'WeightedEnsembleModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', [])                : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "('int', [])                  : 1 | ['passenger_count']\n",
      "('int', ['datetime_as_int']) : 5 | ['pickup_datetime', 'pickup_datetime.year', 'pickup_datetime.month', 'pickup_datetime.day', 'pickup_datetime.dayofweek']\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marcial\\AppData\\Roaming\\Python\\Python39\\site-packages\\autogluon\\core\\utils\\plots.py:138: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_types': {'KNeighborsUnif': 'KNNModel',\n",
       "  'KNeighborsDist': 'KNNModel',\n",
       "  'LightGBMXT': 'LGBModel',\n",
       "  'LightGBM': 'LGBModel',\n",
       "  'RandomForestMSE': 'RFModel',\n",
       "  'CatBoost': 'CatBoostModel',\n",
       "  'ExtraTreesMSE': 'XTModel',\n",
       "  'NeuralNetFastAI': 'NNFastAiTabularModel',\n",
       "  'XGBoost': 'XGBoostModel',\n",
       "  'NeuralNetTorch': 'TabularNeuralNetTorchModel',\n",
       "  'LightGBMLarge': 'LGBModel',\n",
       "  'WeightedEnsemble_L2': 'WeightedEnsembleModel'},\n",
       " 'model_performance': {'KNeighborsUnif': -10.790596444856774,\n",
       "  'KNeighborsDist': -12.101352236542452,\n",
       "  'LightGBMXT': -5.3105035541137475,\n",
       "  'LightGBM': -5.356147533141978,\n",
       "  'RandomForestMSE': -5.566027914425568,\n",
       "  'CatBoost': -5.396860166299744,\n",
       "  'ExtraTreesMSE': -5.964686444469093,\n",
       "  'NeuralNetFastAI': -9.551965060974617,\n",
       "  'XGBoost': -5.46231067264841,\n",
       "  'NeuralNetTorch': -6.1793751008337034,\n",
       "  'LightGBMLarge': -5.367221028733642,\n",
       "  'WeightedEnsemble_L2': -5.24045474199896},\n",
       " 'model_best': 'WeightedEnsemble_L2',\n",
       " 'model_paths': {'KNeighborsUnif': 'uber_predictors\\\\models\\\\KNeighborsUnif\\\\',\n",
       "  'KNeighborsDist': 'uber_predictors\\\\models\\\\KNeighborsDist\\\\',\n",
       "  'LightGBMXT': 'uber_predictors\\\\models\\\\LightGBMXT\\\\',\n",
       "  'LightGBM': 'uber_predictors\\\\models\\\\LightGBM\\\\',\n",
       "  'RandomForestMSE': 'uber_predictors\\\\models\\\\RandomForestMSE\\\\',\n",
       "  'CatBoost': 'uber_predictors\\\\models\\\\CatBoost\\\\',\n",
       "  'ExtraTreesMSE': 'uber_predictors\\\\models\\\\ExtraTreesMSE\\\\',\n",
       "  'NeuralNetFastAI': 'uber_predictors\\\\models\\\\NeuralNetFastAI\\\\',\n",
       "  'XGBoost': 'uber_predictors\\\\models\\\\XGBoost\\\\',\n",
       "  'NeuralNetTorch': 'uber_predictors\\\\models\\\\NeuralNetTorch\\\\',\n",
       "  'LightGBMLarge': 'uber_predictors\\\\models\\\\LightGBMLarge\\\\',\n",
       "  'WeightedEnsemble_L2': 'uber_predictors\\\\models\\\\WeightedEnsemble_L2\\\\'},\n",
       " 'model_fit_times': {'KNeighborsUnif': 1.5591814517974854,\n",
       "  'KNeighborsDist': 0.6015458106994629,\n",
       "  'LightGBMXT': 29.708836555480957,\n",
       "  'LightGBM': 6.062787771224976,\n",
       "  'RandomForestMSE': 93.07080578804016,\n",
       "  'CatBoost': 55.51107907295227,\n",
       "  'ExtraTreesMSE': 24.092205286026,\n",
       "  'NeuralNetFastAI': 138.2293610572815,\n",
       "  'XGBoost': 6.620389461517334,\n",
       "  'NeuralNetTorch': 134.55720329284668,\n",
       "  'LightGBMLarge': 8.252931356430054,\n",
       "  'WeightedEnsemble_L2': 0.26329684257507324},\n",
       " 'model_pred_times': {'KNeighborsUnif': 0.012140989303588867,\n",
       "  'KNeighborsDist': 0.010244369506835938,\n",
       "  'LightGBMXT': 0.4837064743041992,\n",
       "  'LightGBM': 0.046874284744262695,\n",
       "  'RandomForestMSE': 0.0688173770904541,\n",
       "  'CatBoost': 0.004011631011962891,\n",
       "  'ExtraTreesMSE': 0.06484723091125488,\n",
       "  'NeuralNetFastAI': 0.032912254333496094,\n",
       "  'XGBoost': 0.02593088150024414,\n",
       "  'NeuralNetTorch': 0.012966394424438477,\n",
       "  'LightGBMLarge': 0.05186200141906738,\n",
       "  'WeightedEnsemble_L2': 0.0009975433349609375},\n",
       " 'num_bag_folds': 0,\n",
       " 'max_stack_level': 2,\n",
       " 'model_hyperparams': {'KNeighborsUnif': {'weights': 'uniform'},\n",
       "  'KNeighborsDist': {'weights': 'distance'},\n",
       "  'LightGBMXT': {'learning_rate': 0.05, 'extra_trees': True},\n",
       "  'LightGBM': {'learning_rate': 0.05},\n",
       "  'RandomForestMSE': {'n_estimators': 300,\n",
       "   'max_leaf_nodes': 15000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'bootstrap': True,\n",
       "   'criterion': 'squared_error'},\n",
       "  'CatBoost': {'iterations': 10000,\n",
       "   'learning_rate': 0.05,\n",
       "   'random_seed': 0,\n",
       "   'allow_writing_files': False,\n",
       "   'eval_metric': 'RMSE'},\n",
       "  'ExtraTreesMSE': {'n_estimators': 300,\n",
       "   'max_leaf_nodes': 15000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'bootstrap': True,\n",
       "   'criterion': 'squared_error'},\n",
       "  'NeuralNetFastAI': {'layers': None,\n",
       "   'emb_drop': 0.1,\n",
       "   'ps': 0.1,\n",
       "   'bs': 'auto',\n",
       "   'lr': 0.01,\n",
       "   'epochs': 'auto',\n",
       "   'early.stopping.min_delta': 0.0001,\n",
       "   'early.stopping.patience': 20,\n",
       "   'smoothing': 0.0},\n",
       "  'XGBoost': {'n_estimators': 10000,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_jobs': -1,\n",
       "   'proc.max_category_levels': 100,\n",
       "   'objective': 'reg:squarederror',\n",
       "   'booster': 'gbtree'},\n",
       "  'NeuralNetTorch': {'num_epochs': 500,\n",
       "   'epochs_wo_improve': 20,\n",
       "   'activation': 'relu',\n",
       "   'embedding_size_factor': 1.0,\n",
       "   'embed_exponent': 0.56,\n",
       "   'max_embedding_dim': 100,\n",
       "   'y_range': None,\n",
       "   'y_range_extend': 0.05,\n",
       "   'dropout_prob': 0.1,\n",
       "   'optimizer': 'adam',\n",
       "   'learning_rate': 0.0003,\n",
       "   'weight_decay': 1e-06,\n",
       "   'proc.embed_min_categories': 4,\n",
       "   'proc.impute_strategy': 'median',\n",
       "   'proc.max_category_levels': 100,\n",
       "   'proc.skew_threshold': 0.99,\n",
       "   'use_ngram_features': False,\n",
       "   'num_layers': 4,\n",
       "   'hidden_size': 128,\n",
       "   'max_batch_size': 512,\n",
       "   'use_batchnorm': False,\n",
       "   'loss_function': 'auto'},\n",
       "  'LightGBMLarge': {'learning_rate': 0.03,\n",
       "   'num_leaves': 128,\n",
       "   'feature_fraction': 0.9,\n",
       "   'min_data_in_leaf': 5},\n",
       "  'WeightedEnsemble_L2': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True}},\n",
       " 'leaderboard':                   model  score_val  pred_time_val    fit_time  \\\n",
       " 0   WeightedEnsemble_L2  -5.240455       0.665224  271.915862   \n",
       " 1            LightGBMXT  -5.310504       0.483706   29.708837   \n",
       " 2              LightGBM  -5.356148       0.046874    6.062788   \n",
       " 3         LightGBMLarge  -5.367221       0.051862    8.252931   \n",
       " 4              CatBoost  -5.396860       0.004012   55.511079   \n",
       " 5               XGBoost  -5.462311       0.025931    6.620389   \n",
       " 6       RandomForestMSE  -5.566028       0.068817   93.070806   \n",
       " 7         ExtraTreesMSE  -5.964686       0.064847   24.092205   \n",
       " 8        NeuralNetTorch  -6.179375       0.012966  134.557203   \n",
       " 9       NeuralNetFastAI  -9.551965       0.032912  138.229361   \n",
       " 10       KNeighborsUnif -10.790596       0.012141    1.559181   \n",
       " 11       KNeighborsDist -12.101352       0.010244    0.601546   \n",
       " \n",
       "     pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       " 0                 0.000998           0.263297            2       True   \n",
       " 1                 0.483706          29.708837            1       True   \n",
       " 2                 0.046874           6.062788            1       True   \n",
       " 3                 0.051862           8.252931            1       True   \n",
       " 4                 0.004012          55.511079            1       True   \n",
       " 5                 0.025931           6.620389            1       True   \n",
       " 6                 0.068817          93.070806            1       True   \n",
       " 7                 0.064847          24.092205            1       True   \n",
       " 8                 0.012966         134.557203            1       True   \n",
       " 9                 0.032912         138.229361            1       True   \n",
       " 10                0.012141           1.559181            1       True   \n",
       " 11                0.010244           0.601546            1       True   \n",
       " \n",
       "     fit_order  \n",
       " 0          12  \n",
       " 1           3  \n",
       " 2           4  \n",
       " 3          11  \n",
       " 4           6  \n",
       " 5           9  \n",
       " 6           5  \n",
       " 7           7  \n",
       " 8          10  \n",
       " 9           8  \n",
       " 10          1  \n",
       " 11          2  }"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d227a7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WeightedEnsemble_L2'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.get_model_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327e9b03",
   "metadata": {},
   "source": [
    "The best result is the **WeightedEnsemble_L2**.\n",
    "This is, as the name states, not a unique model, but an ensemble consisting of the best models.\n",
    "During inference, the result is stored for each individual model and then the average is returned as final prediction. You will often see the Weighted Ensemble L2 model as the best model for superbvised learning tasks, which makes sense, as it leverages the best aspects of multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ef509b",
   "metadata": {},
   "source": [
    "### Model Loading\n",
    "To load a model, you can simply call the **load(save_path)** function provided by the TabularPredictor class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef26d316",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'uber_predictors'\n",
    "predictor = TabularPredictor.load(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2190e04",
   "metadata": {},
   "source": [
    "### Validation\n",
    "We again store the test labels separately and drop them from the test dataset before performing inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "035d76db",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_data[\"fare_amount\"]\n",
    "test_data = test_data.drop(columns=[\"fare_amount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac067eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: root_mean_squared_error on test data: -4.516020470232751\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -4.516020470232751,\n",
      "    \"mean_squared_error\": -20.394440887561238,\n",
      "    \"mean_absolute_error\": -1.9806206680879594,\n",
      "    \"r2\": 0.7858434394747308,\n",
      "    \"pearsonr\": 0.8871483667510556,\n",
      "    \"median_absolute_error\": -1.1549899578094478\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "y_pred = predictor.predict(test_data)\n",
    "metrics = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367a4f9e",
   "metadata": {},
   "source": [
    "### Feature Processing\n",
    "By using **predictor.transform_features()** we can take a look the the feature engineering autogluon performs.\n",
    "We can see that it converts *pickup_datetime* into the columns year, month, day and dayofweek.\n",
    "It automatically recognized that this column contains a timestamp!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37d07d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_datetime.year</th>\n",
       "      <th>pickup_datetime.month</th>\n",
       "      <th>pickup_datetime.day</th>\n",
       "      <th>pickup_datetime.dayofweek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181147</th>\n",
       "      <td>-73.973382</td>\n",
       "      <td>40.758758</td>\n",
       "      <td>-73.975726</td>\n",
       "      <td>40.752431</td>\n",
       "      <td>1</td>\n",
       "      <td>1412111412000000000</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11872</th>\n",
       "      <td>-73.991657</td>\n",
       "      <td>40.749813</td>\n",
       "      <td>-73.975553</td>\n",
       "      <td>40.754817</td>\n",
       "      <td>2</td>\n",
       "      <td>1285239392000000000</td>\n",
       "      <td>2010</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59780</th>\n",
       "      <td>-74.015345</td>\n",
       "      <td>40.715857</td>\n",
       "      <td>-73.984415</td>\n",
       "      <td>40.754312</td>\n",
       "      <td>6</td>\n",
       "      <td>1333468260000000000</td>\n",
       "      <td>2012</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134326</th>\n",
       "      <td>-73.968078</td>\n",
       "      <td>40.762517</td>\n",
       "      <td>-73.988275</td>\n",
       "      <td>40.752637</td>\n",
       "      <td>2</td>\n",
       "      <td>1396358100000000000</td>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21277</th>\n",
       "      <td>-73.987672</td>\n",
       "      <td>40.722537</td>\n",
       "      <td>-73.985028</td>\n",
       "      <td>40.678245</td>\n",
       "      <td>2</td>\n",
       "      <td>1247980560000000000</td>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85963</th>\n",
       "      <td>-73.974993</td>\n",
       "      <td>40.753044</td>\n",
       "      <td>-73.951787</td>\n",
       "      <td>40.766452</td>\n",
       "      <td>2</td>\n",
       "      <td>1290439762000000000</td>\n",
       "      <td>2010</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122272</th>\n",
       "      <td>-73.972308</td>\n",
       "      <td>40.745573</td>\n",
       "      <td>-73.776505</td>\n",
       "      <td>40.645037</td>\n",
       "      <td>1</td>\n",
       "      <td>1248681660000000000</td>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67094</th>\n",
       "      <td>-74.001332</td>\n",
       "      <td>40.729155</td>\n",
       "      <td>-73.988511</td>\n",
       "      <td>40.737315</td>\n",
       "      <td>1</td>\n",
       "      <td>1355663532000000000</td>\n",
       "      <td>2012</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14101</th>\n",
       "      <td>-73.989483</td>\n",
       "      <td>40.720613</td>\n",
       "      <td>-73.990403</td>\n",
       "      <td>40.750275</td>\n",
       "      <td>1</td>\n",
       "      <td>1246581480000000000</td>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66238</th>\n",
       "      <td>-73.998195</td>\n",
       "      <td>40.745543</td>\n",
       "      <td>-73.964765</td>\n",
       "      <td>40.791767</td>\n",
       "      <td>1</td>\n",
       "      <td>1278757980000000000</td>\n",
       "      <td>2010</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "181147        -73.973382        40.758758         -73.975726   \n",
       "11872         -73.991657        40.749813         -73.975553   \n",
       "59780         -74.015345        40.715857         -73.984415   \n",
       "134326        -73.968078        40.762517         -73.988275   \n",
       "21277         -73.987672        40.722537         -73.985028   \n",
       "...                  ...              ...                ...   \n",
       "85963         -73.974993        40.753044         -73.951787   \n",
       "122272        -73.972308        40.745573         -73.776505   \n",
       "67094         -74.001332        40.729155         -73.988511   \n",
       "14101         -73.989483        40.720613         -73.990403   \n",
       "66238         -73.998195        40.745543         -73.964765   \n",
       "\n",
       "        dropoff_latitude  passenger_count      pickup_datetime  \\\n",
       "181147         40.752431                1  1412111412000000000   \n",
       "11872          40.754817                2  1285239392000000000   \n",
       "59780          40.754312                6  1333468260000000000   \n",
       "134326         40.752637                2  1396358100000000000   \n",
       "21277          40.678245                2  1247980560000000000   \n",
       "...                  ...              ...                  ...   \n",
       "85963          40.766452                2  1290439762000000000   \n",
       "122272         40.645037                1  1248681660000000000   \n",
       "67094          40.737315                1  1355663532000000000   \n",
       "14101          40.750275                1  1246581480000000000   \n",
       "66238          40.791767                1  1278757980000000000   \n",
       "\n",
       "        pickup_datetime.year  pickup_datetime.month  pickup_datetime.day  \\\n",
       "181147                  2014                      9                   30   \n",
       "11872                   2010                      9                   23   \n",
       "59780                   2012                      4                    3   \n",
       "134326                  2014                      4                    1   \n",
       "21277                   2009                      7                   19   \n",
       "...                      ...                    ...                  ...   \n",
       "85963                   2010                     11                   22   \n",
       "122272                  2009                      7                   27   \n",
       "67094                   2012                     12                   16   \n",
       "14101                   2009                      7                    3   \n",
       "66238                   2010                      7                   10   \n",
       "\n",
       "        pickup_datetime.dayofweek  \n",
       "181147                          1  \n",
       "11872                           3  \n",
       "59780                           1  \n",
       "134326                          1  \n",
       "21277                           6  \n",
       "...                           ...  \n",
       "85963                           0  \n",
       "122272                          0  \n",
       "67094                           6  \n",
       "14101                           4  \n",
       "66238                           5  \n",
       "\n",
       "[2500 rows x 10 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.transform_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19de6986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
       "       'dropoff_latitude', 'passenger_count', 'pickup_datetime',\n",
       "       'pickup_datetime.year', 'pickup_datetime.month', 'pickup_datetime.day',\n",
       "       'pickup_datetime.dayofweek'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.transform_features().columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6640ce",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9315709c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 6 features using 5000 rows with 5 shuffle sets...\n",
      "\t84.1s\t= Expected runtime (16.82s per shuffle set)\n",
      "\t66.91s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>stddev</th>\n",
       "      <th>p_value</th>\n",
       "      <th>n</th>\n",
       "      <th>p99_high</th>\n",
       "      <th>p99_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <td>6.194479</td>\n",
       "      <td>0.579849</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>5</td>\n",
       "      <td>7.388395</td>\n",
       "      <td>5.000563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <td>5.857570</td>\n",
       "      <td>0.619943</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>5</td>\n",
       "      <td>7.134042</td>\n",
       "      <td>4.581098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_latitude</th>\n",
       "      <td>4.159462</td>\n",
       "      <td>0.522828</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>5</td>\n",
       "      <td>5.235971</td>\n",
       "      <td>3.082952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_longitude</th>\n",
       "      <td>3.865383</td>\n",
       "      <td>0.513659</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>5</td>\n",
       "      <td>4.923014</td>\n",
       "      <td>2.807753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_datetime</th>\n",
       "      <td>1.463497</td>\n",
       "      <td>0.377773</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>5</td>\n",
       "      <td>2.241337</td>\n",
       "      <td>0.685658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>passenger_count</th>\n",
       "      <td>0.094713</td>\n",
       "      <td>0.034545</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>5</td>\n",
       "      <td>0.165842</td>\n",
       "      <td>0.023583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   importance    stddev   p_value  n  p99_high   p99_low\n",
       "dropoff_latitude     6.194479  0.579849  0.000009  5  7.388395  5.000563\n",
       "dropoff_longitude    5.857570  0.619943  0.000015  5  7.134042  4.581098\n",
       "pickup_latitude      4.159462  0.522828  0.000029  5  5.235971  3.082952\n",
       "pickup_longitude     3.865383  0.513659  0.000037  5  4.923014  2.807753\n",
       "pickup_datetime      1.463497  0.377773  0.000489  5  2.241337  0.685658\n",
       "passenger_count      0.094713  0.034545  0.001794  5  0.165842  0.023583"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.feature_importance(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a744f",
   "metadata": {},
   "source": [
    "## Inference Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dee16c",
   "metadata": {},
   "source": [
    "If fast inference is an important criteria, you can pass it to autogluon as a training constraint.\n",
    "Let's say, that inference speed has to be smaller than 6ms on a single datapoint\n",
    "\n",
    "If you receive an **Impossible to satisfy inference constraint** error, try increasing the time limit by a small amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a4236da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"uber_fast\"\n",
      "infer_limit specified, but infer_limit_batch_size was not specified. Setting infer_limit_batch_size=10000\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (180000 samples, 28.71 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"uber_fast\\\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.12\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Train Data Rows:    180000\n",
      "Train Data Columns: 6\n",
      "Label Column: fare_amount\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (499.0, -52.0, 11.36729, 9.91752)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    23272.7 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.6 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                        : 1 | ['passenger_count']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['pickup_datetime']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                  : 1 | ['passenger_count']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['pickup_datetime', 'pickup_datetime.year', 'pickup_datetime.month', 'pickup_datetime.day', 'pickup_datetime.dayofweek']\n",
      "\t15.9s = Fit runtime\n",
      "\t6 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 14.4 MB (0.1% of available memory)\n",
      "\t0.087ms\t= Feature Preprocessing Time (1 row | 10000 batch size)\n",
      "\t\tFeature Preprocessing requires 14.53% of the overall inference constraint (0.6ms)\n",
      "\t\t0.513ms inference time budget remaining for models...\n",
      "Data preprocessing and feature engineering runtime = 16.84s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.013888888888888888, Train Rows: 177500, Val Rows: 2500\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-10.7906\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.65s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "\t2.199μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t2.199μs\t = Validation runtime (1 row | 10000 batch size)\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-12.1014\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.67s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "\t2.094μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t2.094μs\t = Validation runtime (1 row | 10000 batch size)\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 5.57015\n",
      "[2000]\tvalid_set's rmse: 5.42349\n",
      "[3000]\tvalid_set's rmse: 5.38171\n",
      "[4000]\tvalid_set's rmse: 5.33706\n",
      "[5000]\tvalid_set's rmse: 5.32888\n",
      "[6000]\tvalid_set's rmse: 5.31629\n",
      "[7000]\tvalid_set's rmse: 5.31203\n",
      "[8000]\tvalid_set's rmse: 5.31692\n",
      "[9000]\tvalid_set's rmse: 5.31871\n",
      "[10000]\tvalid_set's rmse: 5.3241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-5.3105\t = Validation score   (-root_mean_squared_error)\n",
      "\t33.64s\t = Training   runtime\n",
      "\t0.47s\t = Validation runtime\n",
      "\t0.202ms\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t0.202ms\t = Validation runtime (1 row | 10000 batch size)\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 5.3808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-5.3561\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.41s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "\t0.017ms\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t0.017ms\t = Validation runtime (1 row | 10000 batch size)\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-5.566\t = Validation score   (-root_mean_squared_error)\n",
      "\t94.85s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "\t0.019ms\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t0.019ms\t = Validation runtime (1 row | 10000 batch size)\n",
      "Fitting model: CatBoost ...\n",
      "\t-5.3969\t = Validation score   (-root_mean_squared_error)\n",
      "\t56.81s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "\t0.738μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t0.738μs\t = Validation runtime (1 row | 10000 batch size)\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-5.9647\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.21s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "\t0.017ms\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t0.017ms\t = Validation runtime (1 row | 10000 batch size)\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-9.552\t = Validation score   (-root_mean_squared_error)\n",
      "\t171.41s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "\t0.017ms\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t0.017ms\t = Validation runtime (1 row | 10000 batch size)\n",
      "Fitting model: XGBoost ...\n",
      "\t-5.4623\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.41s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "\t9.475μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t9.475μs\t = Validation runtime (1 row | 10000 batch size)\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-6.1794\t = Validation score   (-root_mean_squared_error)\n",
      "\t162.39s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "\t2.892μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t2.892μs\t = Validation runtime (1 row | 10000 batch size)\n",
      "Fitting model: LightGBMLarge ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 5.37828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-5.3672\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.55s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "\t0.021ms\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t0.021ms\t = Validation runtime (1 row | 10000 batch size)\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-5.2405\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "\t0.179μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t0.262ms\t = Validation runtime (1 row | 10000 batch size)\n",
      "AutoGluon training complete, total runtime = 590.0s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"uber_fast\\\")\n"
     ]
    }
   ],
   "source": [
    "infer_time_limit = 0.0006 # INCREASE IF NECESSARY, ESPECIALLY ON SLOWER COMPUTERS!!\n",
    "fast_predictor = TabularPredictor(label=\"fare_amount\", path=\"uber_fast\").fit(\n",
    "    train_data,\n",
    "    infer_limit=infer_time_limit,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f51c23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: root_mean_squared_error on test data: -4.516020470232751\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -4.516020470232751,\n",
      "    \"mean_squared_error\": -20.394440887561238,\n",
      "    \"mean_absolute_error\": -1.9806206680879594,\n",
      "    \"r2\": 0.7858434394747308,\n",
      "    \"pearsonr\": 0.8871483667510556,\n",
      "    \"median_absolute_error\": -1.1549899578094478\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "y_pred = fast_predictor.predict(test_data)\n",
    "metrics = fast_predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0ca6f",
   "metadata": {},
   "source": [
    "## Manual hyperparameters\n",
    "\n",
    "You can of course pass fixed hyperparameters for some (or all models) during training.It should be noted the defaults are usually quite good, and unless you are very familiar with both the data set and the model, you shouldn't expect performance increases based on your own parameter choices.\n",
    "Let's say you only want to train the neural networks for one epoch and use a learning rate of 1e-2. This should yeild **worse** results (we do this to confirm AutoGluon is using our parameters, in general, its actually sometimes a bit hard to find parameters that would perform **better** than the defaults!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85f95a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_nn = {\"num_epochs\": 1, \"learning_rate\": 1e-2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9edbb6a",
   "metadata": {},
   "source": [
    "We assign the neural network hyperparameters to **NN_TORCH** as we want autogluon to only apply them on the neural network implemented in pytorch.\n",
    "\n",
    "As you can see in the official documentation, the model name is **NN_TORCH** and not *NeuralNetTorch* as stated above.(https://auto.gluon.ai/stable/api/autogluon.tabular.TabularPredictor.fit.html#:~:text=Keys%20are%20strings%20that%20indicate%20which%20model%20types%20to%20train)\n",
    "\n",
    "\"’GBM’ (LightGBM) ‘CAT’ (CatBoost) ‘XGB’ (XGBoost) ‘RF’ (random forest) ‘XT’ (extremely randomized trees) ‘KNN’ (k-nearest neighbors) ‘LR’ (linear regression) ‘NN_TORCH’ (neural network implemented in Pytorch) ‘FASTAI’ (neural network with FastAI backend) ‘AG_AUTOMM’ (MultimodalPredictor from autogluon.multimodal. Supports Tabular, Text, and Image modalities. GPU is required.)\"\n",
    "\n",
    "The reference to all hyperparameters for all models can be found here: \n",
    "https://auto.gluon.ai/stable/api/autogluon.tabular.TabularPredictor.fit.html#:~:text=Details%20regarding%20the%20hyperparameters%20you%20can%20specify%20for%20each%20model%20are%20provided%20in%20the%20following%20files%3A\n",
    "\n",
    "You need to go to the specified file (e.g on github) where you finally can find all hyperparameters\n",
    "https://github.com/autogluon/autogluon/blob/master/tabular/src/autogluon/tabular/models/tabular_nn/hyperparameters/parameters.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62b5d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"NN_TORCH\": hyperparams_nn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "970d49e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'uber_predictors_manual'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db8ba7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TabularPredictor(label=\"fare_amount\", path=save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "41993851",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (180000 samples, 28.71 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"uber_predictors_manual\\\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.12\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Train Data Rows:    180000\n",
      "Train Data Columns: 6\n",
      "Label Column: fare_amount\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (499.0, -52.0, 11.36729, 9.91752)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    18883.98 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.6 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                        : 1 | ['passenger_count']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['pickup_datetime']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                  : 1 | ['passenger_count']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['pickup_datetime', 'pickup_datetime.year', 'pickup_datetime.month', 'pickup_datetime.day', 'pickup_datetime.dayofweek']\n",
      "\t15.9s = Fit runtime\n",
      "\t6 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 14.4 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 15.97s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.013888888888888888, Train Rows: 177500, Val Rows: 2500\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-64.5959\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-64.5959\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 19.34s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"uber_predictors_manual\\\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x1d28cbf9070>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.fit(train_data, hyperparameters=hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ba2d0a",
   "metadata": {},
   "source": [
    "We can see two things:\n",
    "1. The NeuralNetTorch reaches a validation score of -65 which is significantly worse than above. It used our hyperparameters\n",
    "2. It only uses the models that occur in the hyperparameter dictionary.\n",
    "\n",
    "To train with more models, we need to specify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d537e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"NN_TORCH\": hyperparams_nn, \"XGB\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ba7b363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"uber_predictors_manual\"\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label=\"fare_amount\", path=save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d2ea734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (180000 samples, 28.71 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"uber_predictors_manual\\\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.12\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Train Data Rows:    180000\n",
      "Train Data Columns: 6\n",
      "Label Column: fare_amount\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (499.0, -52.0, 11.36729, 9.91752)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    18882.15 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.6 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                        : 1 | ['passenger_count']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['pickup_datetime']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                  : 1 | ['passenger_count']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['pickup_datetime', 'pickup_datetime.year', 'pickup_datetime.month', 'pickup_datetime.day', 'pickup_datetime.dayofweek']\n",
      "\t15.8s = Fit runtime\n",
      "\t6 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 14.4 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 15.9s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.013888888888888888, Train Rows: 177500, Val Rows: 2500\n",
      "Fitting 2 L1 models ...\n",
      "Fitting model: XGBoost ...\n",
      "\t-5.4623\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.43s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-64.5959\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.47s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-5.4623\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 26.01s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"uber_predictors_manual\\\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x1d29f05b250>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.fit(train_data, hyperparameters=hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0e081f",
   "metadata": {},
   "source": [
    "## Manual validation set during training\n",
    "\n",
    "If you have strong insights in your dataset it might be helpful to specify some specific validation data to use during training. However, in all other cases, you should rely on autogluon as it uses advanced strategies such as stratified sampling to select the validation data.\n",
    "\n",
    "Let's define a train-val-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3315fca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: data/uber/uber.csv | Columns = 7 / 7 | Rows = 200000 -> 200000\n"
     ]
    }
   ],
   "source": [
    "data = TabularDataset(\"data/uber/uber.csv\")\n",
    "\n",
    "validation_data = data.head(1000)\n",
    "remaining_data = data.tail(-1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "32ae4191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.5</td>\n",
       "      <td>2015-05-07 19:52:06 UTC</td>\n",
       "      <td>-73.999817</td>\n",
       "      <td>40.738354</td>\n",
       "      <td>-73.999512</td>\n",
       "      <td>40.723217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2009-07-17 20:04:56 UTC</td>\n",
       "      <td>-73.994355</td>\n",
       "      <td>40.728225</td>\n",
       "      <td>-73.994710</td>\n",
       "      <td>40.750325</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.9</td>\n",
       "      <td>2009-08-24 21:45:00 UTC</td>\n",
       "      <td>-74.005043</td>\n",
       "      <td>40.740770</td>\n",
       "      <td>-73.962565</td>\n",
       "      <td>40.772647</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.3</td>\n",
       "      <td>2009-06-26 08:22:21 UTC</td>\n",
       "      <td>-73.976124</td>\n",
       "      <td>40.790844</td>\n",
       "      <td>-73.965316</td>\n",
       "      <td>40.803349</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2014-08-28 17:47:00 UTC</td>\n",
       "      <td>-73.925023</td>\n",
       "      <td>40.744085</td>\n",
       "      <td>-73.973082</td>\n",
       "      <td>40.761247</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2011-05-04 06:39:00 UTC</td>\n",
       "      <td>-73.969720</td>\n",
       "      <td>40.757577</td>\n",
       "      <td>-73.953782</td>\n",
       "      <td>40.766960</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>8.1</td>\n",
       "      <td>2011-11-23 20:43:20 UTC</td>\n",
       "      <td>-73.993784</td>\n",
       "      <td>40.757054</td>\n",
       "      <td>-73.980018</td>\n",
       "      <td>40.775632</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>8.5</td>\n",
       "      <td>2010-01-11 20:58:00 UTC</td>\n",
       "      <td>-73.972338</td>\n",
       "      <td>40.765078</td>\n",
       "      <td>-73.954527</td>\n",
       "      <td>40.783833</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2013-06-12 17:01:24 UTC</td>\n",
       "      <td>-73.979054</td>\n",
       "      <td>40.784730</td>\n",
       "      <td>-73.982970</td>\n",
       "      <td>40.775048</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>9.7</td>\n",
       "      <td>2011-12-11 21:48:22 UTC</td>\n",
       "      <td>-73.983675</td>\n",
       "      <td>40.729944</td>\n",
       "      <td>-73.992416</td>\n",
       "      <td>40.758208</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     fare_amount          pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
       "0            7.5  2015-05-07 19:52:06 UTC        -73.999817        40.738354   \n",
       "1            7.7  2009-07-17 20:04:56 UTC        -73.994355        40.728225   \n",
       "2           12.9  2009-08-24 21:45:00 UTC        -74.005043        40.740770   \n",
       "3            5.3  2009-06-26 08:22:21 UTC        -73.976124        40.790844   \n",
       "4           16.0  2014-08-28 17:47:00 UTC        -73.925023        40.744085   \n",
       "..           ...                      ...               ...              ...   \n",
       "995          5.7  2011-05-04 06:39:00 UTC        -73.969720        40.757577   \n",
       "996          8.1  2011-11-23 20:43:20 UTC        -73.993784        40.757054   \n",
       "997          8.5  2010-01-11 20:58:00 UTC        -73.972338        40.765078   \n",
       "998          5.5  2013-06-12 17:01:24 UTC        -73.979054        40.784730   \n",
       "999          9.7  2011-12-11 21:48:22 UTC        -73.983675        40.729944   \n",
       "\n",
       "     dropoff_longitude  dropoff_latitude  passenger_count  \n",
       "0           -73.999512         40.723217                1  \n",
       "1           -73.994710         40.750325                1  \n",
       "2           -73.962565         40.772647                1  \n",
       "3           -73.965316         40.803349                3  \n",
       "4           -73.973082         40.761247                5  \n",
       "..                 ...               ...              ...  \n",
       "995         -73.953782         40.766960                1  \n",
       "996         -73.980018         40.775632                3  \n",
       "997         -73.954527         40.783833                5  \n",
       "998         -73.982970         40.775048                1  \n",
       "999         -73.992416         40.758208                1  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "477f0f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>8.9</td>\n",
       "      <td>2012-06-15 18:02:33 UTC</td>\n",
       "      <td>-73.952845</td>\n",
       "      <td>40.768120</td>\n",
       "      <td>-73.976917</td>\n",
       "      <td>40.779899</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2013-11-12 12:41:00 UTC</td>\n",
       "      <td>-73.997807</td>\n",
       "      <td>40.745105</td>\n",
       "      <td>-73.986150</td>\n",
       "      <td>40.734672</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2014-12-21 10:53:16 UTC</td>\n",
       "      <td>-73.953874</td>\n",
       "      <td>40.778966</td>\n",
       "      <td>-73.952112</td>\n",
       "      <td>40.791495</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>5.3</td>\n",
       "      <td>2010-03-28 23:21:00 UTC</td>\n",
       "      <td>-73.991827</td>\n",
       "      <td>40.729355</td>\n",
       "      <td>-73.997550</td>\n",
       "      <td>40.741742</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>20.0</td>\n",
       "      <td>2014-03-20 21:41:28 UTC</td>\n",
       "      <td>-74.007019</td>\n",
       "      <td>40.730269</td>\n",
       "      <td>-73.996877</td>\n",
       "      <td>40.694110</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2012-10-28 10:49:00 UTC</td>\n",
       "      <td>-73.987042</td>\n",
       "      <td>40.739367</td>\n",
       "      <td>-73.986525</td>\n",
       "      <td>40.740297</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>7.5</td>\n",
       "      <td>2014-03-14 01:09:00 UTC</td>\n",
       "      <td>-73.984722</td>\n",
       "      <td>40.736837</td>\n",
       "      <td>-74.006672</td>\n",
       "      <td>40.739620</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>30.9</td>\n",
       "      <td>2009-06-29 00:42:00 UTC</td>\n",
       "      <td>-73.986017</td>\n",
       "      <td>40.756487</td>\n",
       "      <td>-73.858957</td>\n",
       "      <td>40.692588</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>14.5</td>\n",
       "      <td>2015-05-20 14:56:25 UTC</td>\n",
       "      <td>-73.997124</td>\n",
       "      <td>40.725452</td>\n",
       "      <td>-73.983215</td>\n",
       "      <td>40.695415</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>14.1</td>\n",
       "      <td>2010-05-15 04:08:00 UTC</td>\n",
       "      <td>-73.984395</td>\n",
       "      <td>40.720077</td>\n",
       "      <td>-73.985508</td>\n",
       "      <td>40.768793</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        fare_amount          pickup_datetime  pickup_longitude  \\\n",
       "1000            8.9  2012-06-15 18:02:33 UTC        -73.952845   \n",
       "1001            7.0  2013-11-12 12:41:00 UTC        -73.997807   \n",
       "1002            6.0  2014-12-21 10:53:16 UTC        -73.953874   \n",
       "1003            5.3  2010-03-28 23:21:00 UTC        -73.991827   \n",
       "1004           20.0  2014-03-20 21:41:28 UTC        -74.007019   \n",
       "...             ...                      ...               ...   \n",
       "199995          3.0  2012-10-28 10:49:00 UTC        -73.987042   \n",
       "199996          7.5  2014-03-14 01:09:00 UTC        -73.984722   \n",
       "199997         30.9  2009-06-29 00:42:00 UTC        -73.986017   \n",
       "199998         14.5  2015-05-20 14:56:25 UTC        -73.997124   \n",
       "199999         14.1  2010-05-15 04:08:00 UTC        -73.984395   \n",
       "\n",
       "        pickup_latitude  dropoff_longitude  dropoff_latitude  passenger_count  \n",
       "1000          40.768120         -73.976917         40.779899                1  \n",
       "1001          40.745105         -73.986150         40.734672                1  \n",
       "1002          40.778966         -73.952112         40.791495                1  \n",
       "1003          40.729355         -73.997550         40.741742                1  \n",
       "1004          40.730269         -73.996877         40.694110                1  \n",
       "...                 ...                ...               ...              ...  \n",
       "199995        40.739367         -73.986525         40.740297                1  \n",
       "199996        40.736837         -74.006672         40.739620                1  \n",
       "199997        40.756487         -73.858957         40.692588                2  \n",
       "199998        40.725452         -73.983215         40.695415                1  \n",
       "199999        40.720077         -73.985508         40.768793                1  \n",
       "\n",
       "[199000 rows x 7 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d250a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 180000\n",
    "seed = 42\n",
    "\n",
    "train_data = remaning_data.sample(train_size, random_state=seed)\n",
    "test_data = remaning_data.drop(train_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "29f82518",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'uber_predictors_manual_val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "91b012ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TabularPredictor(label=\"fare_amount\", path=save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e86d1ba",
   "metadata": {},
   "source": [
    "We now pass our validation_data to **predictor.fit()** using *tuning_data* argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "feee0241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (180000 samples, 24.48 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"uber_predictors_manual_val\\\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.12\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Train Data Rows:    180000\n",
      "Train Data Columns: 6\n",
      "Tuning Data Rows:    1000\n",
      "Tuning Data Columns: 6\n",
      "Label Column: fare_amount\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (499.0, -52.0, 11.35872, 9.9161)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    18807.51 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.72 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                        : 1 | ['passenger_count']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['pickup_datetime']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                  : 1 | ['passenger_count']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['pickup_datetime', 'pickup_datetime.year', 'pickup_datetime.month', 'pickup_datetime.day', 'pickup_datetime.dayofweek']\n",
      "\t16.2s = Fit runtime\n",
      "\t6 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 14.48 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 16.3s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-10.2069\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-11.0997\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 3.9057\n",
      "[2000]\tvalid_set's rmse: 3.87218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-3.8664\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.41s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 3.86138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-3.8462\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-3.7658\t = Validation score   (-root_mean_squared_error)\n",
      "\t96.83s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-3.6903\t = Validation score   (-root_mean_squared_error)\n",
      "\t76.15s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-4.6866\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.76s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-6.8238\t = Validation score   (-root_mean_squared_error)\n",
      "\t148.92s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-4.048\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.2s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-3.7919\t = Validation score   (-root_mean_squared_error)\n",
      "\t244.32s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t-3.95\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-3.5775\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 631.65s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"uber_predictors_manual_val\\\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x1d2980341f0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.fit(train_data, tuning_data=validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f39eb4",
   "metadata": {},
   "source": [
    "### Final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "08bc893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_data[\"fare_amount\"]\n",
    "test_data = test_data.drop(columns=[\"fare_amount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "86b26b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predictor.predict(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "59d7e2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: root_mean_squared_error on test data: -4.443080196188303\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -4.443080196188303,\n",
      "    \"mean_squared_error\": -19.74096162976069,\n",
      "    \"mean_absolute_error\": -1.9083817665451452,\n",
      "    \"r2\": 0.7944643933260358,\n",
      "    \"pearsonr\": 0.8915248996677385,\n",
      "    \"median_absolute_error\": -1.1130312919616698\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "metrics = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e88e978-3845-4405-8c30-afd218ca1b8f",
   "metadata": {},
   "source": [
    "# Great Job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154fcd4d-8e5a-45a3-ab0a-7dd38c246f96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
