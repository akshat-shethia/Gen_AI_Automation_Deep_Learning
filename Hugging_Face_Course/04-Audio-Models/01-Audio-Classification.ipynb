{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72566782-321f-4e1c-9ae4-8572b6fa99df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Audio Classification\n",
    "\n",
    "# Libraries\n",
    "\n",
    "Balancing, torch, torchaudio, and transformers can be tricky! Here are the versions used for this notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a531a21a-2753-4230-94f1-4c4c3b11bb85",
   "metadata": {},
   "source": [
    "## Library and Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "458a072c-f306-49c7-9af4-675a1850fe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the versions used for this notebook, but watch the lecture for an important note on this\n",
      "2.3.0+cpu\n",
      "2.3.0+cpu\n",
      "4.44.2\n"
     ]
    }
   ],
   "source": [
    "import torch, transformers, torchaudio\n",
    "print(\"These are the versions used for this notebook, but watch the lecture for an important note on this\")\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe422402-cac1-4962-9d91-be9af4b7e77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, ASTForAudioClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5d0307d-bb51-40fe-bf3a-bb99501aff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d06e7008-d918-4f2a-84f1-ed9700d00e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "audio_path = 'example.mp3'\n",
    "y, sr = librosa.load(audio_path, sr=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee2059-3f2b-4273-8f84-e4bcaae2419d",
   "metadata": {},
   "source": [
    "## Sampling Rate Issues\n",
    "\n",
    "Recall that most ML models are trained on 16 kHz sampling rate, you will run into issues if you try to force your own sampling rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da8caf3d-0511-457f-a1a3-377921e358a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR!\n",
    "\n",
    "# You can leave the sampling_rate column if your sampling rate doesnt match 16000,\n",
    "# the model will automatically upsample / downsample to 16000 Hz\n",
    "\n",
    "# result = feature_extractor(y,sampling_rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c87cec69-240e-44d9-897b-f7ae5c08b052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    "result = feature_extractor(y,return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "728a1b1f-61fc-4111-afc5-bb186e706b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': tensor([[[ 0.0770, -0.2676,  0.1092,  ..., -1.2776, -1.2776, -1.2776],\n",
       "         [ 0.0846, -0.2771,  0.0997,  ..., -1.2776, -1.2776, -1.2776],\n",
       "         [-0.2939, -0.3674,  0.0095,  ..., -1.2776, -1.2776, -1.2776],\n",
       "         ...,\n",
       "         [ 0.2184, -0.0845,  0.2923,  ..., -1.2776, -1.2776, -1.2776],\n",
       "         [ 0.1963, -0.1293,  0.2475,  ..., -1.2776, -1.2776, -1.2776],\n",
       "         [-0.0509, -0.4521, -0.0752,  ..., -1.2776, -1.2776, -1.2776]]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae49bafd-8bb0-408b-b5bf-1ed7d4e254aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e4efed1-1818-403b-b32d-5653d84b5b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_logits = model(result['input_values']).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98da0a86-5902-49a3-b062-de29467861dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -5.9081,  -9.2253,  -9.4442,  -9.4990, -10.3686,  -9.8169, -10.5372,\n",
       "          -9.9140, -11.0119, -11.1294, -10.8732, -11.3651, -12.3890, -11.9322,\n",
       "         -10.1947, -10.5203, -10.9214, -11.4615, -11.2874, -10.9701, -11.3812,\n",
       "         -11.1792, -10.5879, -10.7083,  -9.4158, -11.2141, -10.9959,  -8.1345,\n",
       "          -8.3063, -12.1890,  -8.9141,  -8.1373,  -9.5586, -10.1567, -10.2660,\n",
       "         -11.6801, -10.3482, -10.8475, -10.4001, -10.8535, -11.0698,  -9.5788,\n",
       "         -12.0086, -10.2645, -10.1869, -10.8037, -10.6147, -10.8702, -11.1634,\n",
       "         -11.6032, -12.3197, -11.4073, -10.7273, -10.6266, -10.7189, -10.2814,\n",
       "         -12.3984, -10.6907, -11.4477, -10.8105, -12.4599, -10.8371, -11.6523,\n",
       "         -11.1757,  -8.7801,  -9.1707, -11.2029, -10.2863, -11.8267,  -9.5817,\n",
       "         -10.8891, -11.8293,  -7.8608,  -8.4752,  -9.2617, -10.5787, -10.6426,\n",
       "         -10.5211,  -9.6369, -11.0325,  -9.8193,  -9.8407, -11.3109, -10.0103,\n",
       "          -9.1413, -11.3183,  -9.6363, -10.5181, -10.5326, -12.0045,  -9.7329,\n",
       "         -10.8257, -11.1573, -10.9969, -10.1537, -11.7148, -10.4393, -10.0457,\n",
       "          -9.5154, -10.3393, -11.1337, -11.2132, -10.6031, -10.6171,  -9.9654,\n",
       "         -10.8855, -10.8607, -10.2859, -10.3977, -10.0613, -10.8958,  -8.9154,\n",
       "         -10.0099, -11.0398, -11.1266, -10.3692, -10.9305, -10.7355, -10.7680,\n",
       "         -10.6131, -11.3413, -11.0807, -10.5832, -11.4765, -11.0102, -11.1487,\n",
       "          -9.4741, -11.3131, -11.3831, -10.1499,  -8.8026,  -9.8782, -10.2924,\n",
       "         -11.2721, -10.7637, -10.0475,  -9.4250,   0.4300,  -1.4319,  -6.8755,\n",
       "          -6.6131,  -8.5558,  -8.5463,  -7.5936, -10.0831, -10.4967,  -8.0951,\n",
       "         -10.5235, -10.2498,  -8.7156,  -8.7834, -10.2167,  -5.4794,  -4.7773,\n",
       "          -8.1011,  -6.5816,  -8.6970,  -7.7294,  -7.4143,  -8.5919,  -6.7687,\n",
       "          -7.3252,  -7.8707, -10.0673,  -7.6365,  -9.8580,  -9.6902, -10.7651,\n",
       "          -9.0910,  -5.2914, -10.8723,  -9.9249, -10.1679, -10.1487, -11.4788,\n",
       "         -11.7280, -12.4099,  -7.3504,  -9.1572,  -9.2087,  -8.5854,  -8.9355,\n",
       "          -9.1996, -10.8581,  -1.8113,  -6.1373,  -6.0532,  -7.5425,  -6.8150,\n",
       "          -0.6655,  -1.8169,  -1.4128,  -5.8041,   0.2643,  -1.2275,  -7.2777,\n",
       "          -7.7061,  -8.2559,  -8.2045,  -8.4547,  -7.9105,  -8.4694,  -9.8173,\n",
       "         -12.0253, -10.9005, -10.6103, -11.9745,  -9.4159,  -9.0896,  -8.8592,\n",
       "          -9.5125,  -8.1735, -10.2054,  -7.6791,  -8.1452, -10.6568,  -9.0703,\n",
       "          -9.3680, -11.0343,  -9.0656,  -8.3357,  -9.1140,  -9.4543,  -9.1140,\n",
       "          -8.8625,  -9.4616,  -8.1090,  -8.2514, -11.0697,  -7.6292,  -9.9742,\n",
       "          -8.8073,  -9.2797,  -7.9875,  -7.9516,  -7.4070, -10.1921,  -1.9498,\n",
       "          -6.1480,  -6.6403,  -9.2608,  -8.2486,  -8.2451,  -9.9719,  -8.3496,\n",
       "          -9.3053,  -6.4909,  -8.6586,  -9.4429, -11.3152,  -8.1062,  -8.0532,\n",
       "          -9.8256,  -7.0520,  -9.9942, -10.9314, -10.3782, -10.9846,  -8.1131,\n",
       "          -9.3158,  -9.2393,  -9.0825, -11.2661, -10.4387,  -7.9818,  -7.6349,\n",
       "          -9.6584,  -4.9590,  -4.2996,  -9.2699,  -4.3060,  -7.6594,  -6.6592,\n",
       "          -8.0987,  -9.0688,  -5.8876,  -8.4456,  -8.3315,  -4.3573,  -5.8473,\n",
       "          -6.6354,  -8.5419,  -3.6140,  -9.8581,  -9.6094,  -9.6057, -10.2131,\n",
       "         -10.0596,  -9.7069,  -9.9575, -10.3185,  -9.9919, -10.0574, -10.4357,\n",
       "          -9.9153, -10.0350, -10.4551, -10.0486, -10.9560, -11.2392,  -6.1035,\n",
       "          -8.3096,  -9.8094, -10.6017,  -9.4351,  -9.3881,  -8.8800,  -7.3255,\n",
       "          -9.5954, -10.0896, -11.8437, -11.9944, -11.1438, -11.3778,  -9.4987,\n",
       "         -10.0680,  -9.5604, -10.9194, -10.8022, -10.8105, -11.7541,  -8.4908,\n",
       "         -10.3638, -10.6319, -11.1494, -10.3253,  -9.7755,  -9.7374,  -8.4655,\n",
       "          -8.2878, -11.1102,  -9.7361,  -8.3484, -10.6815, -10.0333,  -8.7086,\n",
       "          -9.7731, -10.5014,  -9.5137, -10.5344,  -8.6966, -10.2862, -11.9116,\n",
       "          -8.8287, -10.7487, -10.9666, -10.5173, -11.3691,  -9.9821,  -9.7356,\n",
       "         -10.7288, -10.8355,  -9.8492,  -9.3838, -10.5110, -11.7532, -10.4866,\n",
       "         -11.1524, -10.6830, -11.6753,  -9.2969, -11.2406, -10.9046, -12.9853,\n",
       "         -10.7980, -12.0079, -11.0965, -11.2718,  -8.5802, -11.3292, -11.1634,\n",
       "         -11.5378, -10.6431, -10.9956, -11.7193, -11.3479, -11.7661, -11.1956,\n",
       "         -10.2958, -10.9797, -11.4412, -12.1931, -11.3892, -11.9881, -10.2856,\n",
       "         -11.5581,  -9.8520,  -9.9037, -10.8921, -10.9863, -11.2686, -10.0095,\n",
       "         -11.4514, -10.3062, -10.2749, -10.9382,  -9.3200,  -9.6347, -11.1500,\n",
       "         -11.2691, -11.3676,  -7.6472, -11.2387, -11.3087,  -9.0157, -10.5773,\n",
       "         -11.0836,  -9.8965,  -9.5430, -10.5482, -12.2448, -10.7935, -10.5800,\n",
       "          -9.7354, -11.6099,  -9.8166, -11.4525, -10.3505,  -9.7880,  -9.6779,\n",
       "         -11.3579, -11.7079, -11.6343, -11.8440, -10.5128, -11.4875,  -9.7587,\n",
       "          -8.7508, -10.0785, -10.9720,  -9.8802, -11.5815, -11.2129, -11.5732,\n",
       "          -8.9675, -10.0057, -10.0783, -10.0697, -11.3742, -13.4724, -12.2799,\n",
       "          -9.9220, -10.2079, -10.7791, -10.3969, -12.2077, -11.6577, -10.6070,\n",
       "         -11.1305, -11.1907, -11.2188, -10.7186, -11.0600,  -9.8152, -11.9005,\n",
       "         -12.3758, -11.8247,  -8.5626, -11.5660,  -8.5471,  -9.4409,  -9.7390,\n",
       "          -9.8128,  -9.8606, -10.2806, -11.4655,  -9.1085, -10.7802,  -9.9842,\n",
       "         -10.8448, -10.4663, -10.6041, -11.9202,  -9.8174, -12.0070, -11.2889,\n",
       "         -10.8205,  -9.5464, -11.4868, -12.3960, -11.6334,  -9.4803,  -9.7509,\n",
       "          -9.8122, -11.0075, -12.3329, -12.5127,  -9.1825, -10.2188,  -9.4983,\n",
       "         -11.3232, -10.9226, -10.5472,  -6.2590, -11.8984,  -9.9241,  -8.3434,\n",
       "          -9.9370, -12.2206, -10.4559,  -7.5244,  -9.7453,  -7.5909, -10.0599,\n",
       "          -7.2087,  -9.3882,  -6.8283,  -6.1235,  -8.0279,  -7.0175,  -6.9616,\n",
       "          -8.9820,  -7.8229, -11.0230, -11.1495, -10.1394,  -9.0847, -10.3326,\n",
       "         -10.7166,  -9.7585,  -8.9684, -10.1934,  -8.1415,  -9.4875,  -8.7257,\n",
       "         -10.2586,  -8.4603]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3dfc19c-b0ab-4b67-95c5-ee9df7361908",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_ids = torch.argmax(prediction_logits, dim=-1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e792e681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c23e65a9-6d79-4e3c-88ed-d425f0b835f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = model.config.id2label[predicted_class_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a86578c-bba3-48b3-be8d-e28aa5a17776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Music'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f843c03b-cf0c-4922-98bb-dbe07d5af1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbe6d98-87b8-4086-95c1-d96bf40536d8",
   "metadata": {},
   "source": [
    "## Pipeline for Audio Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7bb1d37-859f-4cee-8979-aea1ab77038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"audio-classification\", model=\"MIT/ast-finetuned-audioset-10-10-0.4593\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d725f7ef-cc90-433d-b5da-af0b141a9cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ASTForAudioClassification(\n",
       "  (audio_spectrogram_transformer): ASTModel(\n",
       "    (embeddings): ASTEmbeddings(\n",
       "      (patch_embeddings): ASTPatchEmbeddings(\n",
       "        (projection): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ASTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ASTLayer(\n",
       "          (attention): ASTSdpaAttention(\n",
       "            (attention): ASTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): ASTMLPHead(\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dense): Linear(in_features=768, out_features=527, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "160631d9-bcc8-4771-a43b-56cbd6d428a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ffmpeg was not found but is required to load audio files from filename",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\miniconda\\Gen_AI_Automation_Deep_Learning\\Hugging_Face_Course\\venv\\Lib\\site-packages\\transformers\\pipelines\\audio_classification.py:54\u001b[0m, in \u001b[0;36mffmpeg_read\u001b[1;34m(bpayload, sampling_rate)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     ffmpeg_process \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mffmpeg_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\miniconda\\Gen_AI_Automation_Deep_Learning\\Hugging_Face_Course\\venv\\Lib\\subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\miniconda\\Gen_AI_Automation_Deep_Learning\\Hugging_Face_Course\\venv\\Lib\\subprocess.py:1538\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1538\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1540\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1547\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexample.mp3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\Gen_AI_Automation_Deep_Learning\\Hugging_Face_Course\\venv\\Lib\\site-packages\\transformers\\pipelines\\audio_classification.py:136\u001b[0m, in \u001b[0;36mAudioClassificationPipeline.__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    105\u001b[0m     inputs: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    107\u001b[0m ):\n\u001b[0;32m    108\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m    Classify the sequence(s) given as inputs. See the [`AutomaticSpeechRecognitionPipeline`] documentation for more\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;124;03m    information.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m        - **score** (`float`) -- The corresponding probability.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\Gen_AI_Automation_Deep_Learning\\Hugging_Face_Course\\venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1257\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1250\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1251\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m         )\n\u001b[0;32m   1255\u001b[0m     )\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\Gen_AI_Automation_Deep_Learning\\Hugging_Face_Course\\venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1263\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m-> 1263\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1264\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1265\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[1;32mc:\\miniconda\\Gen_AI_Automation_Deep_Learning\\Hugging_Face_Course\\venv\\Lib\\site-packages\\transformers\\pipelines\\audio_classification.py:158\u001b[0m, in \u001b[0;36mAudioClassificationPipeline.preprocess\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    155\u001b[0m             inputs \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m--> 158\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mffmpeg_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# Accepting `\"array\"` which is the key defined in `datasets` for\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# better integration\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampling_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs)):\n",
      "File \u001b[1;32mc:\\miniconda\\Gen_AI_Automation_Deep_Learning\\Hugging_Face_Course\\venv\\Lib\\site-packages\\transformers\\pipelines\\audio_classification.py:56\u001b[0m, in \u001b[0;36mffmpeg_read\u001b[1;34m(bpayload, sampling_rate)\u001b[0m\n\u001b[0;32m     54\u001b[0m     ffmpeg_process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(ffmpeg_command, stdin\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE, stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mffmpeg was not found but is required to load audio files from filename\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m output_stream \u001b[38;5;241m=\u001b[39m ffmpeg_process\u001b[38;5;241m.\u001b[39mcommunicate(bpayload)\n\u001b[0;32m     58\u001b[0m out_bytes \u001b[38;5;241m=\u001b[39m output_stream[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: ffmpeg was not found but is required to load audio files from filename"
     ]
    }
   ],
   "source": [
    "pipe('example.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d3ea93-a1b1-4272-b42d-87a04964f0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "527"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pipe.model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36b69fa-ac10-49a7-9282-c93b4de70a58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
